# Agent Memory Bank: Critical Learnings & Patterns

> **Purpose**: Living document of team learnings to prevent repeating failures and replicate successes
> **Usage**: Scan before starting work on any agent development or CI/CD issues
> **Last Updated**: 2025-01-07

## üö® Critical Failures to Remember

### 1. The "handof" Typo Disaster (CodeQL Alert #75)
**What Happened**: Misspelled "handoff" as "handof" in context-manager.py causing:
- CodeQL security alert for "Unused import"
- Method signature error: `handof()` doesn't exist
- CI failures despite local tests passing

**Code That Failed**:
```python
# WRONG - caused CodeQL error
def handof(self):  # Typo!
    pass

# Import became "unused" because method name was wrong
from .context_manager import ContextManager
```

**Lesson**: ALWAYS verify method names match exactly. Typos in method signatures create cascade failures.

**Prevention Command**:
```bash
# Before committing, verify all method calls match definitions
grep -r "def " . | grep -E "(handof|handoff)"
```

### 2. Local Validation Lying About Compliance
**What Happened**: Local technical debt checker showed 0 violations, CI showed 2052
- Local: "‚úì No technical debt found (0 violations)"
- CI: "‚úó Technical debt found (2052 violations)"
- Wasted hours debugging "phantom" failures

**Root Cause**: Different environments, different file scopes, different configurations

**Lesson**: Local validation can lie. CI is the source of truth.

**Prevention Commands**:
```bash
# Create diagnostic tool to compare local vs expected CI behavior
python ci-local-diagnostic.py
# Always run this when local passes but CI fails

# Double-check with multiple tools
python tools/validation/check-technical-debt.py --verbose
find . -name "*.py" -exec grep -l "TODO\|FIXME\|HACK" {} \;
```

### 3. Magic Number Patching Attempts
**What Happened**: Tried to "fix" 2052 violations by:
- Adding magic exclusion numbers
- Patching specific violation counts
- Hardcoding thresholds instead of understanding root cause

**Wrong Approach**:
```python
# WRONG - magic number patching
if violations == 2052:
    return 0  # Ignore these specific violations
```

**Right Approach**:
```python
# RIGHT - mathematical formula
T = L √ó R √ó C √ó M  # Total = Lines √ó Risk √ó Complexity √ó Maintenance
threshold = calculate_threshold(project_metrics)
```

**Lesson**: Never patch symptoms. Always understand and fix root causes with systematic approaches.

### 4. Overconfidence When Local Passes
**What Happened**: Assumed local success meant CI success
- "Local validation passed, so CI should pass"
- Multiple failed CI runs due to environment differences
- Wasted time on wrong assumptions

**Lesson**: Environment differences are real. Local success ‚â† CI success.

## üèÜ Success Patterns to Replicate

### 1. Mathematical Formula Approach
**What Worked**: Replaced magic numbers with calculated thresholds
```python
def calculate_technical_debt_threshold(project_metrics):
    """Calculate threshold using mathematical formula, not magic numbers"""
    lines_of_code = project_metrics.get('lines', 1000)
    risk_factor = project_metrics.get('risk', 0.1)
    complexity = project_metrics.get('complexity', 1.5)
    maintenance = project_metrics.get('maintenance', 2.0)

    # T = L √ó R √ó C √ó M formula
    threshold = int(lines_of_code * risk_factor * complexity * maintenance)
    return max(threshold, 10)  # Minimum threshold
```

**Why This Works**: Evidence-based, explainable, adaptable to different projects

### 2. Systematic Diagnostic Tools
**What Worked**: Created specific tools to debug CI/local differences
```python
# ci-local-diagnostic.py - compares environments
def compare_environments():
    print("=== Local Environment ===")
    run_local_checks()
    print("=== Expected CI Behavior ===")
    simulate_ci_environment()
    print("=== Differences Found ===")
    highlight_differences()
```

**Why This Works**: Makes invisible problems visible, provides concrete evidence

### 3. Team Collaboration on Root Cause Analysis
**What Worked**: Multiple agents contributing different perspectives:
- Agent 1: Identified the typo
- Agent 2: Diagnosed environment differences
- Agent 3: Proposed mathematical solution
- Agent 4: Validated final approach

**Pattern**: Collaborative debugging > individual assumptions

### 4. Evidence-Based Threshold Calculation
**What Worked**: Used actual project data to calculate realistic thresholds
```bash
# Gather evidence first
find . -name "*.py" | wc -l    # Count Python files
wc -l **/*.py                  # Count total lines
grep -r "TODO" . | wc -l       # Count actual technical debt
```

**Why This Works**: Thresholds based on reality, not arbitrary numbers

## üîß Quick Reference Cards

### Card 1: "PR Failing CI But Local Passes"
**Symptoms**: ‚úì Local validation, ‚úó CI validation
**Check These 5 Things**:
1. **Typos in method names**: `grep -r "def " . | grep -E "(handof|handoff)"`
2. **File scope differences**: Compare files being scanned locally vs CI
3. **Environment variables**: Check CI environment settings
4. **Tool versions**: Verify local tools match CI tool versions
5. **Hidden files**: CI might scan files local ignores (`.github/`, etc.)

**Emergency Commands**:
```bash
# Diagnostic tool
python ci-local-diagnostic.py

# Manual comparison
find . -name "*.py" -exec grep -l "TODO\|FIXME\|HACK" {} \; | wc -l
```

### Card 2: "Magic Number Violations"
**Symptoms**: Hardcoded thresholds, arbitrary limits
**DON'T DO**:
```python
if violations > 2000:  # Magic number!
    return "pass"
```

**DO THIS**:
```python
threshold = calculate_threshold(project_metrics)  # Formula-based
if violations > threshold:
    return f"Failed: {violations} > {threshold}"
```

**Formula Templates**:
- **Linear**: `T = base + (lines * factor)`
- **Logarithmic**: `T = base * log(lines + 1)`
- **Multi-factor**: `T = L √ó R √ó C √ó M`

### Card 3: "CodeQL Argument Errors"
**Symptoms**: "method doesn't exist", "unused import"
**Check**:
1. **Spelling**: Method names exactly match definitions
2. **Arguments**: Parameter counts and types match
3. **Imports**: All imported methods are actually called
4. **Scope**: Methods exist in the imported class/module

**Verification Commands**:
```bash
# Find method definitions
grep -r "def method_name" .

# Find method calls
grep -r "method_name(" .

# Check import usage
grep -r "from .* import .*method_name"
```

### Card 4: "Team Needs Systematic Solution"
**When**: Multiple agents working on complex problem
**Framework**:
1. **Define**: What exactly is the problem?
2. **Diagnose**: What evidence do we have?
3. **Design**: What's the systematic solution?
4. **Deploy**: Implement with verification
5. **Document**: Update memory bank

**Collaboration Commands**:
```bash
# Create shared context
echo "Problem: [description]" > team-context.md
echo "Evidence: [findings]" >> team-context.md
echo "Solution: [approach]" >> team-context.md
```

## üß† Memorable Acronyms & Mnemonics

### VERIFY (Before Any Commit)
- **V**alidate locally first
- **E**xamine for typos and errors
- **R**eview method signatures match
- **I**terate until CI-ready
- **F**inalize with evidence
- **Y**ield to CI as final authority

### MATH (For Threshold Calculation)
- **M**easure actual project metrics
- **A**nalyze patterns and trends
- **T**ransform into mathematical formula
- **H**old accountable with evidence

### CI-FIRST (When Local Passes But CI Fails)
- **C**heck environment differences
- **I**nvestigate with diagnostic tools
- **F**ind root cause with evidence
- **I**terate on systematic solution
- **R**epeat until CI passes
- **S**ave learnings to memory bank
- **T**rust CI over local results

## üîç Proven Diagnostic Commands

### Environment Comparison
```bash
# Compare Python versions
echo "Local: $(python --version)"
echo "CI uses: Check .github/workflows/*.yml"

# Compare file counts
echo "Local Python files: $(find . -name '*.py' | wc -l)"
echo "Files with TODO: $(find . -name '*.py' -exec grep -l 'TODO' {} \; | wc -l)"

# Check tool versions
pip list | grep -E "(flake8|black|mypy|pylint)"
```

### Technical Debt Analysis
```bash
# Count violations by type
grep -r "TODO" --include="*.py" . | wc -l
grep -r "FIXME" --include="*.py" . | wc -l
grep -r "HACK" --include="*.py" . | wc -l
grep -r "XXX" --include="*.py" . | wc -l

# Find largest files (often have most debt)
find . -name "*.py" -exec wc -l {} \; | sort -nr | head -10
```

### CodeQL Preparation
```bash
# Verify all imports are used
python -c "import ast; import sys; [print(f'Unused import in {f}') for f in sys.argv[1:] if check_unused_imports(f)]" **/*.py

# Check method signatures
grep -r "def " --include="*.py" . | grep -v "__"
```

## üìù Code Snippets That Work

### Technical Debt Calculator (Proven)
```python
def calculate_technical_debt_threshold(project_size="medium"):
    """Calculate realistic threshold based on project characteristics"""
    base_thresholds = {
        "small": 50,    # < 1000 lines
        "medium": 150,  # 1000-10000 lines
        "large": 300,   # 10000+ lines
        "enterprise": 500  # Complex enterprise projects
    }
    return base_thresholds.get(project_size, 150)
```

### Environment Diagnostic Tool
```python
def diagnose_ci_local_difference():
    """Compare local environment to CI expectations"""
    import subprocess

    print("=== Local Environment ===")
    print(f"Python: {subprocess.check_output(['python', '--version']).decode().strip()}")
    print(f"Working dir: {os.getcwd()}")

    print("\n=== File Analysis ===")
    py_files = list(Path('.').rglob('*.py'))
    print(f"Total Python files: {len(py_files)}")

    todo_files = []
    for f in py_files:
        if 'TODO' in f.read_text():
            todo_files.append(f)
    print(f"Files with TODO: {len(todo_files)}")

    return {
        'total_files': len(py_files),
        'todo_files': len(todo_files),
        'ratio': len(todo_files) / len(py_files) if py_files else 0
    }
```

## üéØ When to Update This Document

**Add to Critical Failures** when:
- CI fails unexpectedly after local success
- Same type of error occurs multiple times
- Root cause was obvious in hindsight
- Error wasted significant time

**Add to Success Patterns** when:
- Systematic approach solves complex problem
- Mathematical/evidence-based solution works
- Team collaboration yields breakthrough
- Pattern is reusable across different scenarios

**Update Reference Cards** when:
- New common failure pattern emerges
- Better diagnostic commands are discovered
- More efficient workflows are proven
- Acronyms help remember complex procedures

---

**Remember**: This document is LIVING. Update it after every major learning. The goal is preventing future teams from repeating our hard-learned lessons.

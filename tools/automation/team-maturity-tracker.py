#!/usr/bin/env python3
"""
Team Maturity Tracker for AI-First SDLC Framework

Tracks team development progress through an inspiring maturity journey
from Solo Developer to Legendary Team status. Generates motivational
reports that show real teamwork vs theater.
"""

import json
import click
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
from enum import Enum
import os
from collections import defaultdict
import math


class MaturityLevel(Enum):
    """Team maturity levels with inspiring names and clear progression"""

    SOLO = "solo"
    BUILDERS = "builders"
    COLLABORATORS = "collaborators"
    ORCHESTRATORS = "orchestrators"
    INNOVATORS = "innovators"
    LEGENDARY = "legendary"


@dataclass
class MaturityConfig:
    """Configuration for each maturity level"""

    name: str
    title: str
    description: str
    min_team_size: int
    required_practices: List[str]
    celebration_emoji: str
    next_level_preview: str
    milestone_reward: str


@dataclass
class TeamMetrics:
    """Metrics that show real teamwork vs theater"""

    collaboration_score: float
    process_adherence: float
    knowledge_sharing: float
    innovation_rate: float
    delivery_consistency: float
    self_improvement: float
    user_impact: float
    technical_excellence: float


@dataclass
class MilestoneAchievement:
    """Track milestone celebrations and achievements"""

    level: str
    achieved_date: str
    celebration_message: str
    team_size: int
    key_accomplishments: List[str]
    next_goals: List[str]


class TeamMaturityTracker:
    """Tracks and reports on team development maturity"""

    MATURITY_LEVELS = {
        MaturityLevel.SOLO: MaturityConfig(
            name="Solo Developer",
            title="üöÄ Solo Explorer",
            description="Individual mastery of AI-First practices",
            min_team_size=1,
            required_practices=[
                "feature-proposals",
                "retrospectives",
                "architecture-docs",
                "zero-technical-debt",
                "local-validation",
            ],
            celebration_emoji="üéØ",
            next_level_preview="Ready to build with others? Builders level awaits!",
            milestone_reward="Personal Mastery Certificate + SDLC Tool Belt",
        ),
        MaturityLevel.BUILDERS: MaturityConfig(
            name="Team Builders",
            title="üèóÔ∏è Construction Crew",
            description="Small team building things together",
            min_team_size=2,
            required_practices=[
                "all-solo-practices",
                "pair-programming",
                "code-reviews",
                "shared-retrospectives",
                "team-planning",
            ],
            celebration_emoji="üî®",
            next_level_preview="Building great things? Time to master collaboration!",
            milestone_reward="Team Builder Badge + Collaboration Toolkit",
        ),
        MaturityLevel.COLLABORATORS: MaturityConfig(
            name="True Collaborators",
            title="ü§ù Harmony Masters",
            description="Seamless teamwork with shared ownership",
            min_team_size=3,
            required_practices=[
                "all-builder-practices",
                "knowledge-sharing",
                "mentoring",
                "cross-functional-work",
                "team-retrospectives",
                "shared-standards",
            ],
            celebration_emoji="‚ö°",
            next_level_preview="Amazing collaboration! Ready to orchestrate at scale?",
            milestone_reward="Collaboration Champion Award + Mentorship Recognition",
        ),
        MaturityLevel.ORCHESTRATORS: MaturityConfig(
            name="System Orchestrators",
            title="üéº Symphony Conductors",
            description="Coordinating complex systems and multiple teams",
            min_team_size=5,
            required_practices=[
                "all-collaborator-practices",
                "system-thinking",
                "cross-team-coordination",
                "architectural-leadership",
                "process-optimization",
                "scaling-practices",
            ],
            celebration_emoji="üé™",
            next_level_preview="Orchestrating beautifully! Innovation level calls your name!",
            milestone_reward="Orchestra Conductor Medal + System Architecture Recognition",
        ),
        MaturityLevel.INNOVATORS: MaturityConfig(
            name="Innovation Catalysts",
            title="üî• Breakthrough Artists",
            description="Driving industry innovation and setting new standards",
            min_team_size=8,
            required_practices=[
                "all-orchestrator-practices",
                "industry-leadership",
                "open-source-contributions",
                "conference-speaking",
                "research-publications",
                "community-building",
            ],
            celebration_emoji="üöÄ",
            next_level_preview="Revolutionary work! One step from legendary status!",
            milestone_reward="Innovation Pioneer Trophy + Industry Recognition",
        ),
        MaturityLevel.LEGENDARY: MaturityConfig(
            name="Legendary Team",
            title="üëë Hall of Fame",
            description="Setting the standard others aspire to reach",
            min_team_size=10,
            required_practices=[
                "all-innovator-practices",
                "legendary-impact",
                "ecosystem-building",
                "next-generation-mentoring",
                "paradigm-shifting",
                "lasting-legacy",
            ],
            celebration_emoji="üëë",
            next_level_preview="You've reached the pinnacle! Continue inspiring others!",
            milestone_reward="Legendary Status Crown + Eternal Hall of Fame Recognition",
        ),
    }

    def __init__(self, project_path: Path = None):
        self.project_path = project_path or Path.cwd()
        self.tracking_dir = self.project_path / ".sdlc" / "maturity"
        self.tracking_dir.mkdir(parents=True, exist_ok=True)

    def analyze_current_maturity(
            self) -> Tuple[MaturityLevel, TeamMetrics, Dict]:
        """Analyze current team maturity level and metrics"""

        # Gather team data
        team_data = self._gather_team_data()

        # Calculate metrics
        metrics = self._calculate_metrics(team_data)

        # Determine maturity level
        level = self._determine_maturity_level(team_data, metrics)

        return level, metrics, team_data

    def _gather_team_data(self) -> Dict:
        """Gather comprehensive team data for analysis"""
        data = {}

        # Team composition
        data["team_size"] = self._get_team_size()
        data["contributors"] = self._get_recent_contributors()
        data["commit_frequency"] = self._get_commit_frequency()

        # Process adherence
        data["feature_proposals"] = self._count_feature_proposals()
        data["retrospectives"] = self._count_retrospectives()
        data["architecture_docs"] = self._count_architecture_docs()
        data["technical_debt"] = self._assess_technical_debt()

        # Collaboration indicators
        data["code_reviews"] = self._count_code_reviews()
        data["pair_programming"] = self._detect_pair_programming()
        data["knowledge_sharing"] = self._assess_knowledge_sharing()
        data["cross_functional"] = self._assess_cross_functional_work()

        # Innovation metrics
        data["innovation_rate"] = self._calculate_innovation_rate()
        data["external_contributions"] = self._count_external_contributions()
        data["community_impact"] = self._assess_community_impact()

        # Delivery metrics
        data["delivery_consistency"] = self._assess_delivery_consistency()
        data["user_feedback"] = self._assess_user_feedback()
        data["system_reliability"] = self._assess_system_reliability()

        return data

    def _calculate_metrics(self, team_data: Dict) -> TeamMetrics:
        """Calculate normalized metrics from team data"""

        # Collaboration Score (0-100)
        collaboration = min(
            100,
            (
                team_data["code_reviews"] * 10
                + team_data["pair_programming"] * 20
                + team_data["knowledge_sharing"] * 15
                + team_data["cross_functional"] * 25
                + (team_data["team_size"] > 1) * 30
            ),
        )

        # Process Adherence (0-100)
        process = min(
            100,
            (
                (team_data["feature_proposals"] > 0) * 25
                + (team_data["retrospectives"] > 0) * 25
                + (team_data["architecture_docs"] >= 6) * 25
                + (team_data["technical_debt"] < 5) * 25
            ),
        )

        # Knowledge Sharing (0-100)
        knowledge = min(100, team_data["knowledge_sharing"])

        # Innovation Rate (0-100)
        innovation = min(100, team_data["innovation_rate"])

        # Delivery Consistency (0-100)
        delivery = min(100, team_data["delivery_consistency"])

        # Self Improvement (0-100)
        improvement = min(
            100,
            (
                team_data["retrospectives"] * 2
                + (team_data["commit_frequency"] > 0.5) * 30
                + (team_data["technical_debt"] < 3) * 40
            ),
        )

        # User Impact (0-100)
        impact = min(100, team_data.get("user_feedback", 50))

        # Technical Excellence (0-100)
        excellence = min(
            100,
            (
                (team_data["technical_debt"] < 3) * 40
                + (team_data["architecture_docs"] >= 6) * 30
                + team_data.get("system_reliability", 50) * 0.3
            ),
        )

        return TeamMetrics(
            collaboration_score=collaboration,
            process_adherence=process,
            knowledge_sharing=knowledge,
            innovation_rate=innovation,
            delivery_consistency=delivery,
            self_improvement=improvement,
            user_impact=impact,
            technical_excellence=excellence,
        )

    def _determine_maturity_level(
        self, team_data: Dict, metrics: TeamMetrics
    ) -> MaturityLevel:
        """Determine current maturity level based on data and metrics"""

        team_size = team_data["team_size"]
        avg_score = (
            metrics.collaboration_score
            + metrics.process_adherence
            + metrics.knowledge_sharing
            + metrics.innovation_rate
            + metrics.delivery_consistency
            + metrics.self_improvement
            + metrics.user_impact
            + metrics.technical_excellence
        ) / 8

        # Legendary (95+ score, 10+ team, industry impact)
        if (
            avg_score >= 95
            and team_size >= 10
            and team_data.get("community_impact", 0) >= 90
        ):
            return MaturityLevel.LEGENDARY

        # Innovators (85+ score, 8+ team, external contributions)
        if (
            avg_score >= 85
            and team_size >= 8
            and team_data.get("external_contributions", 0) >= 5
        ):
            return MaturityLevel.INNOVATORS

        # Orchestrators (75+ score, 5+ team, system thinking)
        if avg_score >= 75 and team_size >= 5 and metrics.collaboration_score >= 80:
            return MaturityLevel.ORCHESTRATORS

        # Collaborators (65+ score, 3+ team, true collaboration)
        if (
            avg_score >= 65
            and team_size >= 3
            and metrics.collaboration_score >= 70
            and metrics.knowledge_sharing >= 70
        ):
            return MaturityLevel.COLLABORATORS

        # Builders (50+ score, 2+ team, building together)
        if avg_score >= 50 and team_size >= 2 and team_data["code_reviews"] > 0:
            return MaturityLevel.BUILDERS

        # Solo (any solo developer with basic practices)
        return MaturityLevel.SOLO

    def generate_maturity_report(
        self, level: MaturityLevel, metrics: TeamMetrics, team_data: Dict
    ) -> str:
        """Generate an inspiring maturity report"""

        config = self.MATURITY_LEVELS[level]

        report = []
        report.append(f"# {config.celebration_emoji} Team Maturity Report")
        report.append(f"**Current Level**: {config.title}")
        report.append(f"**Team Status**: {config.description}")
        report.append(
            f"**Assessment Date**: {datetime.now().strftime('%B %d, %Y')}")
        report.append("")

        # Executive Dashboard
        report.append("## üìä Team Excellence Dashboard")
        report.append("")

        # Metrics visualization
        metrics_dict = asdict(metrics)
        for metric_name, score in metrics_dict.items():
            metric_display = metric_name.replace("_", " ").title()
            bar = self._create_progress_bar(score, 100)
            grade = self._score_to_grade(score)
            report.append(
                f"**{metric_display}**: {bar} {score:.1f}% ({grade})")

        report.append("")

        # Overall Assessment
        avg_score = sum(metrics_dict.values()) / len(metrics_dict)
        overall_grade = self._score_to_grade(avg_score)
        report.append(
            f"üéØ **Overall Team Maturity**: {avg_score:.1f}% ({overall_grade})"
        )
        report.append("")

        # What's Working Well
        report.append("## ‚úÖ What's Working Brilliantly")
        strengths = self._identify_strengths(metrics_dict, team_data)
        for strength in strengths:
            report.append(f"- {strength}")
        report.append("")

        # Growth Opportunities
        report.append("## üå± Growth Opportunities")
        opportunities = self._identify_opportunities(
            metrics_dict, team_data, level)
        for opportunity in opportunities:
            report.append(f"- {opportunity}")
        report.append("")

        # Next Level Preview
        if level != MaturityLevel.LEGENDARY:
            next_level = self._get_next_level(level)
            next_config = self.MATURITY_LEVELS[next_level]
            report.append("## üéØ Your Next Adventure Awaits")
            report.append(f"**Next Level**: {next_config.title}")
            report.append(f"{config.next_level_preview}")
            report.append("")

            # Readiness assessment
            readiness = self._assess_next_level_readiness(
                level, metrics, team_data)
            report.append(f"**Readiness Score**: {readiness['score']:.1f}%")
            report.append("")

            if readiness["ready_items"]:
                report.append("‚úÖ **Already Achieved**:")
                for item in readiness["ready_items"]:
                    report.append(f"  - {item}")
                report.append("")

            if readiness["todo_items"]:
                report.append("üìã **Path to Next Level**:")
                for item in readiness["todo_items"]:
                    report.append(f"  - {item}")
                report.append("")
        else:
            report.append("## üëë Legendary Status Achieved!")
            report.append(
                "You've reached the pinnacle! Your team sets the standard others aspire to."
            )
            report.append(
                "Focus on mentoring the next generation and leaving a lasting legacy."
            )
            report.append("")

        # Team Spotlight
        report.append("## üåü Team Spotlight")
        spotlight = self._generate_team_spotlight(team_data)
        report.extend(spotlight)
        report.append("")

        # Celebration Section
        if self._should_celebrate(level, team_data):
            report.append("## üéâ Celebration Time!")
            celebration = self._generate_celebration(level, team_data)
            report.extend(celebration)
            report.append("")

        return "\n".join(report)

    def _create_progress_bar(
        self, value: float, max_value: float, width: int = 20
    ) -> str:
        """Create a visual progress bar"""
        filled = int((value / max_value) * width)
        return f"{'‚ñà' * filled}{'‚ñë' * (width - filled)}"

    def _score_to_grade(self, score: float) -> str:
        """Convert numeric score to letter grade"""
        if score >= 95:
            return "A+"
        if score >= 90:
            return "A"
        if score >= 85:
            return "A-"
        if score >= 80:
            return "B+"
        if score >= 75:
            return "B"
        if score >= 70:
            return "B-"
        if score >= 65:
            return "C+"
        if score >= 60:
            return "C"
        if score >= 55:
            return "C-"
        return "D"

    def _identify_strengths(
        self, metrics: Dict[str, float], team_data: Dict
    ) -> List[str]:
        """Identify team strengths based on high-scoring metrics"""
        strengths = []

        # Sort metrics by score to find top performers
        sorted_metrics = sorted(
            metrics.items(),
            key=lambda x: x[1],
            reverse=True)

        for metric, score in sorted_metrics[:3]:  # Top 3 metrics
            if score >= 70:  # Only celebrate good scores
                metric_name = metric.replace("_", " ").title()
                if score >= 95:
                    strengths.append(
                        f"üèÜ **Exceptional {metric_name}**: World-class performance!"
                    )
                elif score >= 85:
                    strengths.append(
                        f"‚≠ê **Outstanding {metric_name}**: Industry-leading results!"
                    )
                elif score >= 75:
                    strengths.append(
                        f"‚ú® **Strong {metric_name}**: Impressive consistency!"
                    )
                else:
                    strengths.append(
                        f"üëç **Good {metric_name}**: Solid foundation!")

        # Add specific achievements
        if team_data["team_size"] >= 5:
            strengths.append(
                "ü§ù **Team Scale**: Successfully coordinating larger team!")
        if team_data["technical_debt"] < 3:
            strengths.append(
                "üíé **Code Quality**: Maintaining excellent technical standards!"
            )
        if team_data.get("external_contributions", 0) > 0:
            strengths.append(
                "üåç **Community Impact**: Contributing beyond your own projects!"
            )

        return strengths or [
            "üå± **Getting Started**: Building foundation for future excellence!"
        ]

    def _identify_opportunities(
        self, metrics: Dict[str, float], team_data: Dict, level: MaturityLevel
    ) -> List[str]:
        """Identify growth opportunities based on lower-scoring metrics"""
        opportunities = []

        # Sort metrics by score to find improvement areas
        sorted_metrics = sorted(metrics.items(), key=lambda x: x[1])

        for metric, score in sorted_metrics[:3]:  # Bottom 3 metrics
            if score < 80:  # Focus on metrics that need improvement
                metric_name = metric.replace("_", " ").title()
                if score < 40:
                    opportunities.append(
                        f"üéØ **{metric_name}**: Prime opportunity for breakthrough improvement!"
                    )
                elif score < 60:
                    opportunities.append(
                        f"üìà **{metric_name}**: Ready for the next level of excellence!"
                    )
                else:
                    opportunities.append(
                        f"‚ö° **{metric_name}**: Good foundation, let's make it great!"
                    )

        # Add level-specific opportunities
        config = self.MATURITY_LEVELS[level]
        if team_data["team_size"] < config.min_team_size + 2:
            opportunities.append(
                "üë• **Team Growth**: Consider expanding to unlock new collaboration patterns!"
            )

        if level in [MaturityLevel.SOLO, MaturityLevel.BUILDERS]:
            opportunities.append(
                "üìö **Knowledge Sharing**: Document and share learnings more actively!"
            )

        if level in [MaturityLevel.COLLABORATORS, MaturityLevel.ORCHESTRATORS]:
            opportunities.append(
                "üî¨ **Innovation**: Experiment with cutting-edge practices!"
            )

        return opportunities or [
            "üöÄ **Keep Improving**: Continue building on your strong foundation!"
        ]

    def _get_next_level(self, current_level: MaturityLevel) -> MaturityLevel:
        """Get the next maturity level"""
        levels = list(MaturityLevel)
        current_index = levels.index(current_level)
        if current_index < len(levels) - 1:
            return levels[current_index + 1]
        return current_level

    def _assess_next_level_readiness(
            self,
            current_level: MaturityLevel,
            metrics: TeamMetrics,
            team_data: Dict) -> Dict:
        """Assess readiness for the next maturity level"""
        next_level = self._get_next_level(current_level)
        next_config = self.MATURITY_LEVELS[next_level]

        ready_items = []
        todo_items = []
        total_checks = 0
        passed_checks = 0

        # Check team size requirement
        total_checks += 1
        if team_data["team_size"] >= next_config.min_team_size:
            ready_items.append(f"Team size ({team_data['team_size']} people)")
            passed_checks += 1
        else:
            needed = next_config.min_team_size - team_data["team_size"]
            todo_items.append(
                f"Grow team by {needed} more member{'s' if needed > 1 else ''}"
            )

        # Check metric thresholds based on level
        metric_thresholds = self._get_level_thresholds(next_level)
        metrics_dict = asdict(metrics)

        for metric, threshold in metric_thresholds.items():
            total_checks += 1
            if metrics_dict[metric] >= threshold:
                ready_items.append(
                    f"{metric.replace('_', ' ').title()} ({metrics_dict[metric]:.1f}%)"
                )
                passed_checks += 1
            else:
                gap = threshold - metrics_dict[metric]
                todo_items.append(
                    f"Improve {metric.replace('_', ' ').title()} by {gap:.1f} points"
                )

        readiness_score = (
            (passed_checks / total_checks * 100) if total_checks > 0 else 0
        )

        return {
            "score": readiness_score,
            "ready_items": ready_items,
            "todo_items": todo_items,
            "total_checks": total_checks,
            "passed_checks": passed_checks,
        }

    def _get_level_thresholds(self, level: MaturityLevel) -> Dict[str, float]:
        """Get metric thresholds for each maturity level"""
        thresholds = {
            MaturityLevel.BUILDERS: {
                "collaboration_score": 40,
                "process_adherence": 50,
                "technical_excellence": 60,
            },
            MaturityLevel.COLLABORATORS: {
                "collaboration_score": 70,
                "knowledge_sharing": 70,
                "process_adherence": 65,
                "technical_excellence": 70,
            },
            MaturityLevel.ORCHESTRATORS: {
                "collaboration_score": 80,
                "knowledge_sharing": 75,
                "process_adherence": 75,
                "delivery_consistency": 75,
                "technical_excellence": 80,
            },
            MaturityLevel.INNOVATORS: {
                "collaboration_score": 85,
                "innovation_rate": 80,
                "user_impact": 85,
                "technical_excellence": 85,
                "delivery_consistency": 80,
            },
            MaturityLevel.LEGENDARY: {
                "collaboration_score": 95,
                "innovation_rate": 90,
                "user_impact": 95,
                "technical_excellence": 95,
                "delivery_consistency": 90,
                "self_improvement": 95,
            },
        }
        return thresholds.get(level, {})

    def _generate_team_spotlight(self, team_data: Dict) -> List[str]:
        """Generate team spotlight section"""
        spotlight = []

        # Team composition
        if team_data["team_size"] == 1:
            spotlight.append(
                "ü¶∏ **Solo Hero**: Mastering the craft with dedication and focus!"
            )
        elif team_data["team_size"] <= 3:
            spotlight.append(
                f"üîß **Tight Crew**: {team_data['team_size']} focused builders creating amazing things!"
            )
        elif team_data["team_size"] <= 7:
            spotlight.append(
                f"‚ö° **Power Squad**: {team_data['team_size']} collaborators firing on all cylinders!"
            )
        else:
            spotlight.append(
                f"üéº **Orchestra**: {team_data['team_size']} talented individuals creating symphonies!"
            )

        # Recent activity
        if team_data.get("commit_frequency", 0) > 1:
            spotlight.append(
                "üî• **High Velocity**: Consistently shipping improvements!")
        elif team_data.get("commit_frequency", 0) > 0.5:
            spotlight.append(
                "üìà **Steady Progress**: Maintaining good development rhythm!"
            )

        # Special achievements
        if team_data.get("technical_debt", 10) < 3:
            spotlight.append(
                "üíé **Quality Masters**: Maintaining exceptional code quality!"
            )
        if team_data.get("external_contributions", 0) > 0:
            spotlight.append(
                "üåç **Community Contributors**: Sharing value with the broader ecosystem!"
            )
        if team_data.get("retrospectives", 0) >= 5:
            spotlight.append(
                "üîÑ **Continuous Learners**: Strong retrospective culture!")

        return spotlight

    def _should_celebrate(self, level: MaturityLevel, team_data: Dict) -> bool:
        """Determine if this is a celebration-worthy moment"""
        # Check if this is a recent level advancement
        last_assessment_file = self.tracking_dir / "last_assessment.json"

        if not last_assessment_file.exists():
            return True  # First assessment, always celebrate

        try:
            with open(last_assessment_file) as f:
                last_assessment = json.load(f)

            # Celebrate if level increased
            if last_assessment.get("level") != level.value:
                return True

            # Celebrate milestone achievements
            last_date = datetime.fromisoformat(
                last_assessment.get("date", "1900-01-01")
            )
            if datetime.now() - last_date > timedelta(days=30):  # Monthly celebrations
                return True

        except Exception:
            return True

        return False

    def _generate_celebration(
            self,
            level: MaturityLevel,
            team_data: Dict) -> List[str]:
        """Generate celebration content"""
        config = self.MATURITY_LEVELS[level]
        celebration = []

        celebration.append(
            f"üéä **Congratulations on reaching {config.title} status!**")
        celebration.append(
            f"{config.celebration_emoji} {config.milestone_reward}")
        celebration.append("")
        celebration.append("**Your Achievement Unlocks:**")

        # Level-specific rewards
        if level == MaturityLevel.SOLO:
            celebration.extend(
                [
                    "- üéØ Personal mastery recognition",
                    "- üìö Access to advanced solo developer resources",
                    "- üèÜ Foundation excellence certificate",
                ]
            )
        elif level == MaturityLevel.BUILDERS:
            celebration.extend(
                [
                    "- üî® Team builder achievement badge",
                    "- ü§ù Collaboration toolkit access",
                    "- üìã Advanced project templates",
                ]
            )
        elif level == MaturityLevel.COLLABORATORS:
            celebration.extend(
                [
                    "- ‚ö° Collaboration champion recognition",
                    "- üéì Mentorship program access",
                    "- üåü Knowledge sharing platform features",
                ]
            )
        elif level == MaturityLevel.ORCHESTRATORS:
            celebration.extend(
                [
                    "- üéº System orchestration mastery",
                    "- üèõÔ∏è Architecture leadership recognition",
                    "- üì° Cross-team coordination tools",
                ]
            )
        elif level == MaturityLevel.INNOVATORS:
            celebration.extend(
                [
                    "- üî• Innovation catalyst status",
                    "- üåç Industry leadership platform",
                    "- üöÄ Research and development resources",
                ]
            )
        elif level == MaturityLevel.LEGENDARY:
            celebration.extend(
                [
                    "- üëë Legendary hall of fame induction",
                    "- ‚ôæÔ∏è Eternal recognition and legacy",
                    "- üåü Next-generation mentorship opportunities",
                ]
            )

        return celebration

    def save_assessment(
        self, level: MaturityLevel, metrics: TeamMetrics, team_data: Dict
    ):
        """Save current assessment for tracking progress"""
        assessment_data = {
            "date": datetime.now().isoformat(),
            "level": level.value,
            "metrics": asdict(metrics),
            "team_data": team_data,
            "framework_version": self._get_framework_version(),
        }

        # Save current assessment
        current_file = self.tracking_dir / "last_assessment.json"
        with open(current_file, "w") as f:
            json.dump(assessment_data, f, indent=2)

        # Append to history
        history_file = self.tracking_dir / "assessment_history.json"
        history = []
        if history_file.exists():
            with open(history_file) as f:
                history = json.load(f)

        history.append(assessment_data)

        # Keep only last 50 assessments
        history = history[-50:]

        with open(history_file, "w") as f:
            json.dump(history, f, indent=2)

    def generate_hall_of_fame_entry(
        self, level: MaturityLevel, metrics: TeamMetrics, team_data: Dict
    ) -> Dict:
        """Generate hall of fame entry for legendary teams"""
        if level != MaturityLevel.LEGENDARY:
            return {}

        return {
            "team_name": team_data.get(
                "team_name",
                "Legendary Team"),
            "achievement_date": datetime.now().isoformat(),
            "team_size": team_data["team_size"],
            "final_scores": asdict(metrics),
            "key_achievements": [
                f"üèÜ Achieved {metrics.collaboration_score:.1f}% collaboration excellence",
                f"üíé Maintained {metrics.technical_excellence:.1f}% technical standards",
                f"üöÄ Delivered {metrics.user_impact:.1f}% user satisfaction",
                f"üîÑ {metrics.innovation_rate:.1f}% innovation rate",
                "üëë Inducted into AI-First SDLC Hall of Fame",
            ],
            "legacy_impact": self._calculate_legacy_impact(team_data),
            "mentorship_influence": team_data.get(
                "teams_mentored",
                0),
            "community_contributions": team_data.get(
                "external_contributions",
                0),
        }

    def _calculate_legacy_impact(self, team_data: Dict) -> str:
        """Calculate the lasting impact of a legendary team"""
        impact_score = (
            team_data.get("teams_mentored", 0) * 10
            + team_data.get("external_contributions", 0) * 5
            + team_data.get("community_impact", 0)
        )

        if impact_score >= 100:
            return "üåç Global ecosystem transformation"
        elif impact_score >= 50:
            return "üè¢ Organizational culture shift"
        elif impact_score >= 25:
            return "üéØ Industry best practice creation"
        else:
            return "‚ú® Team excellence achievement"

    def _get_framework_version(self) -> str:
        """Get current framework version"""
        version_file = self.project_path / "VERSION"
        if version_file.exists():
            return version_file.read_text().strip()
        return "unknown"

    # Data gathering methods (simplified implementations)

    def _get_team_size(self) -> int:
        """Get team size from git contributors"""
        try:
            result = subprocess.run(
                ["git", "log", "--since=30 days ago", "--format=%ae"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                emails = (
                    set(result.stdout.strip().split("\n"))
                    if result.stdout.strip()
                    else set()
                )
                return len(emails)
        except Exception:
            pass
        return 1

    def _get_recent_contributors(self) -> List[str]:
        """Get list of recent contributors"""
        try:
            result = subprocess.run(
                ["git", "log", "--since=30 days ago", "--format=%an", "--no-merges"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                names = (result.stdout.strip().split("\n")
                         if result.stdout.strip() else [])
                return list(set(names))  # Unique names
        except Exception:
            pass
        return []

    def _get_commit_frequency(self) -> float:
        """Get commits per day over last 30 days"""
        try:
            result = subprocess.run(
                ["git", "log", "--since=30 days ago", "--oneline"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                commit_count = (
                    len(result.stdout.strip().split("\n"))
                    if result.stdout.strip()
                    else 0
                )
                return commit_count / 30.0
        except Exception:
            pass
        return 0.0

    def _count_feature_proposals(self) -> int:
        """Count feature proposals"""
        proposal_dir = self.project_path / "docs" / "feature-proposals"
        if proposal_dir.exists():
            return len(list(proposal_dir.glob("*.md")))
        return 0

    def _count_retrospectives(self) -> int:
        """Count retrospectives"""
        retro_dir = self.project_path / "retrospectives"
        if retro_dir.exists():
            return len(list(retro_dir.glob("*.md")))
        return 0

    def _count_architecture_docs(self) -> int:
        """Count architecture documents"""
        arch_dir = self.project_path / "docs" / "architecture"
        if arch_dir.exists():
            return len(list(arch_dir.glob("*.md")))
        return 0

    def _assess_technical_debt(self) -> int:
        """Assess technical debt level"""
        try:
            result = subprocess.run(
                ["python", "tools/validation/check-technical-debt.py", "--count"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return int(result.stdout.strip() or "0")
        except Exception:
            pass
        return 0

    def _count_code_reviews(self) -> int:
        """Count recent code reviews (PR reviews)"""
        # This would integrate with GitHub/GitLab APIs in a real implementation
        # For now, estimate based on merge commits
        try:
            result = subprocess.run(
                ["git", "log", "--since=30 days ago", "--merges", "--oneline"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return (
                    len(result.stdout.strip().split("\n"))
                    if result.stdout.strip()
                    else 0
                )
        except Exception:
            pass
        return 0

    def _detect_pair_programming(self) -> int:
        """Detect pair programming from commit messages"""
        try:
            result = subprocess.run(
                [
                    "git",
                    "log",
                    "--since=30 days ago",
                    "--grep=Co-authored-by",
                    "--oneline",
                ],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return (
                    len(result.stdout.strip().split("\n"))
                    if result.stdout.strip()
                    else 0
                )
        except Exception:
            pass
        return 0

    def _assess_knowledge_sharing(self) -> float:
        """Assess knowledge sharing level"""
        score = 0

        # Documentation quality
        if self._count_architecture_docs() >= 6:
            score += 30

        # README and guides
        if (self.project_path / "README.md").exists():
            score += 20

        # Comment density (rough estimate)
        try:
            result = subprocess.run(
                ["find", ".", "-name", "*.py", "-exec", "grep", "-l", "#", "{}", "+"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                commented_files = (
                    len(result.stdout.strip().split("\n"))
                    if result.stdout.strip()
                    else 0
                )
                if commented_files > 0:
                    score += min(30, commented_files * 3)
        except Exception:
            pass

        # Pair programming evidence
        score += min(20, self._detect_pair_programming() * 5)

        return min(100, score)

    def _assess_cross_functional_work(self) -> float:
        """Assess cross-functional work patterns"""
        # This would analyze commit patterns, file touchpoints, etc.
        # Simplified implementation
        contributors = self._get_recent_contributors()
        if len(contributors) <= 1:
            return 0

        # If multiple contributors, assume some cross-functional work
        return min(100, len(contributors) * 15)

    def _calculate_innovation_rate(self) -> float:
        """Calculate innovation rate"""
        score = 0

        # New feature development
        proposals = self._count_feature_proposals()
        score += min(40, proposals * 8)

        # External contributions
        score += min(30, self._count_external_contributions() * 10)

        # Architecture evolution
        arch_docs = self._count_architecture_docs()
        if arch_docs >= 6:
            score += 20

        # Recent activity
        commit_freq = self._get_commit_frequency()
        if commit_freq > 1:
            score += 10

        return min(100, score)

    def _count_external_contributions(self) -> int:
        """Count external contributions (simplified)"""
        # This would check for open source contributions, etc.
        # For now, check for contribution-related files
        contrib_indicators = [
            "CONTRIBUTING.md",
            "CONTRIBUTORS.md",
            ".github/CONTRIBUTING.md",
            "docs/contributing.md",
            "CHANGELOG.md",
        ]

        count = 0
        for indicator in contrib_indicators:
            if (self.project_path / indicator).exists():
                count += 1

        return count

    def _assess_community_impact(self) -> float:
        """Assess community impact"""
        # This would integrate with various APIs to measure real impact
        # Simplified implementation based on project characteristics
        score = 0

        if (self.project_path / "README.md").exists():
            score += 20

        if (self.project_path / "docs").exists():
            score += 20

        if self._count_external_contributions() > 0:
            score += 30

        # Check for open source indicators
        if (self.project_path / "LICENSE").exists():
            score += 30

        return min(100, score)

    def _assess_delivery_consistency(self) -> float:
        """Assess delivery consistency"""
        try:
            # Check commit regularity over time
            result = subprocess.run(
                ["git", "log", "--since=60 days ago", "--format=%ct"],
                cwd=self.project_path,
                capture_output=True,
                text=True,
            )

            if result.returncode == 0 and result.stdout.strip():
                timestamps = [int(ts)
                              for ts in result.stdout.strip().split("\n")]

                if len(timestamps) < 2:
                    return 50  # Default for low activity

                # Calculate coefficient of variation for commit timing
                intervals = [
                    timestamps[i] - timestamps[i + 1]
                    for i in range(len(timestamps) - 1)
                ]
                if not intervals:
                    return 50

                avg_interval = sum(intervals) / len(intervals)
                if avg_interval == 0:
                    return 100  # All commits at same time - unusual but consistent

                variance = sum(
                    (interval - avg_interval) ** 2 for interval in intervals
                ) / len(intervals)
                std_dev = math.sqrt(variance)
                coef_var = std_dev / avg_interval if avg_interval > 0 else 1

                # Lower coefficient of variation = more consistent
                consistency_score = max(0, 100 - (coef_var * 50))
                return min(100, consistency_score)

        except Exception:
            pass

        return 50  # Default score

    def _assess_user_feedback(self) -> float:
        """Assess user feedback quality"""
        # This would integrate with issue trackers, support systems, etc.
        # Simplified implementation
        score = 75  # Assume good feedback by default

        # Check for issue tracking setup
        if (self.project_path / ".github" / "ISSUE_TEMPLATE").exists():
            score += 10

        # Check for user documentation
        if (self.project_path / "docs" / "user-guide.md").exists():
            score += 15

        return min(100, score)

    def _assess_system_reliability(self) -> float:
        """Assess system reliability"""
        # This would integrate with monitoring systems, error tracking, etc.
        # Simplified implementation based on project health indicators
        score = 70  # Base score

        # Technical debt as reliability indicator
        debt = self._assess_technical_debt()
        if debt < 3:
            score += 20
        elif debt > 10:
            score -= 20

        # Test coverage (if available)
        if (self.project_path / "tests").exists():
            score += 10

        return min(100, max(0, score))


# CLI Interface
@click.group()
def cli():
    """Team Maturity Tracker - Track your journey from Solo to Legendary"""
    pass


@cli.command()
@click.option("--output", "-o", type=click.Path(), help="Save report to file")
@click.option("--json", "output_json", is_flag=True, help="Output as JSON")
def assess(output, output_json):
    """Assess current team maturity level"""
    tracker = TeamMaturityTracker()

    level, metrics, team_data = tracker.analyze_current_maturity()

    if output_json:
        result = {
            "level": level.value,
            "level_name": tracker.MATURITY_LEVELS[level].title,
            "metrics": asdict(metrics),
            "team_data": team_data,
            "assessment_date": datetime.now().isoformat(),
        }
        output_text = json.dumps(result, indent=2)
    else:
        output_text = tracker.generate_maturity_report(
            level, metrics, team_data)

    if output:
        with open(output, "w") as f:
            f.write(output_text)
        click.echo(f"Report saved to {output}")
    else:
        click.echo(output_text)

    # Save assessment
    tracker.save_assessment(level, metrics, team_data)


@cli.command()
def hall_of_fame():
    """View legendary teams hall of fame"""
    click.echo("üëë AI-First SDLC Hall of Fame")
    click.echo("=" * 50)

    # This would load and display hall of fame entries
    click.echo("üåü Coming soon: Legendary teams who've reached the pinnacle!")
    click.echo("")
    click.echo(
        "Be the first to achieve legendary status and claim your place here!")


@cli.command()
def progress():
    """Show progress over time"""
    tracker = TeamMaturityTracker()
    history_file = tracker.tracking_dir / "assessment_history.json"

    if not history_file.exists():
        click.echo("No progress history found. Run 'assess' first!")
        return

    with open(history_file) as f:
        history = json.load(f)

    if not history:
        click.echo("No progress history found.")
        return

    click.echo("üìà Team Maturity Progress")
    click.echo("=" * 30)

    for i, assessment in enumerate(history[-10:]):  # Last 10 assessments
        date = datetime.fromisoformat(assessment["date"]).strftime("%Y-%m-%d")
        level = assessment["level"]
        level_config = tracker.MATURITY_LEVELS[MaturityLevel(level)]

        metrics = assessment["metrics"]
        avg_score = sum(metrics.values()) / len(metrics)

        click.echo(f"{date}: {level_config.title} ({avg_score:.1f}%)")


if __name__ == "__main__":
    cli()

# Research Synthesis: SDLC Knowledge Curator

## Research Methodology

- **Date of research**: 2026-02-08
- **Total searches executed**: 0 (web research tools unavailable)
- **Total sources evaluated**: 12 (internal codebase documentation)
- **Sources included (internal framework docs)**: 12
- **Sources excluded**: N/A
- **Target agent archetype**: Domain Expert (knowledge management specialist)
- **Research areas covered**: 5
- **Identified gaps**: 24 (all web-dependent sub-questions)

**CRITICAL LIMITATION**: Web search (WebSearch) and web content retrieval (WebFetch) tools were unavailable during this research session. This research relies entirely on:
1. Internal AI-First SDLC framework documentation (12 comprehensive files)
2. Established knowledge management principles from training data (pre-January 2025)
3. Pattern extraction from existing framework implementations

**Methodology used**: Since web research was unavailable, this synthesis extracts proven patterns from the AI-First SDLC framework's own implementation of knowledge management, onboarding, pattern libraries, success metrics, and knowledge maintenance. The framework itself demonstrates a mature knowledge curation system through its documentation, agent templates, retrospectives, and maturity tracking â€” effectively providing a working case study.

---

## Area 1: Knowledge Management for Development Teams (2025-2026)

### Key Findings

**Pattern: Hierarchical Documentation Architecture**
The AI-First SDLC framework demonstrates a three-tier knowledge organization model:
- **Tier 1 - Core Instructions** (CLAUDE-CORE.md): Compact, mandatory guidance (88% smaller than deprecated CLAUDE.md)
- **Tier 2 - Specialized Guides** (AGENT-TEMPLATE-GUIDE.md, AGENT-CREATION-GUIDE.md): Domain-specific how-to documentation
- **Tier 3 - Living Examples** (retrospectives/, docs/feature-proposals/): Real-world implementation records

**Source**: /docs/AGENT-TEMPLATE-GUIDE.md, /CLAUDE.md (deprecated marker) [Confidence: HIGH - directly implemented pattern]

**Pattern: Knowledge Discovery Through Agents**
Rather than static documentation search, the framework uses specialized agents (ai-first-kick-starter, sdlc-knowledge-curator) to provide contextual knowledge retrieval:
- Agent receives context-specific query
- Agent surfaces relevant patterns from knowledge base
- Agent recommends related resources and next steps
- Eliminates "documentation graveyard" problem through active curation

**Source**: /agents/sdlc/sdlc-knowledge-curator.md, /docs/AI-FIRST-KICK-STARTER-VISION.md [Confidence: HIGH - production implementation]

**Pattern: Handoff Documentation as Knowledge Capture**
Agent handoff patterns serve dual purpose:
1. Context transfer between agents
2. Knowledge capture of decision-making processes

Template includes:
- Architecture decisions completed
- Cross-agent dependencies identified
- Decision rationale with trade-offs
- Implementation impact documentation

**Source**: /templates/agent-handoff-patterns.md [Confidence: HIGH - documented pattern with examples]

**Pattern: Progressive Disclosure Based on SDLC Level**
Knowledge management adapts to project maturity:
- **Level 1 (Prototype)**: Minimal guidance, focus on intent and retrospectives
- **Level 2 (Production)**: Full documentation requirements, comprehensive patterns
- **Level 3 (Enterprise)**: Complete governance, compliance knowledge

**Source**: /docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md [Confidence: HIGH - framework design principle]

**Pattern: Engineering Handbook Through Agent Catalog**
The framework's agent catalog (68 agents across 5 categories) functions as a living engineering handbook:
- Each agent encapsulates domain expertise
- Agents include core competencies, task instructions, response formats
- Maturity labels (production/stable/beta/stub) indicate knowledge quality
- Agent compositions show how expertise combines for complex scenarios

**Source**: /docs/AGENT-TEMPLATE-GUIDE.md, /retrospectives/51-agent-creation-pipeline.md [Confidence: HIGH - operational system]

**Pattern: Self-Review as Knowledge Quality Gate**
Mandatory self-review process before presenting artifacts:
1. Create complete artifact
2. Review against requirements
3. Revise if needed
4. Iterate until confident
5. Present only reviewed version

This internal quality process prevents low-quality knowledge from entering the system.

**Source**: Templates/CLAUDE.md, /docs/examples/self-review-process.md [Confidence: HIGH - enforced practice]

### Sources

1. **/docs/AGENT-TEMPLATE-GUIDE.md** (280 lines) - Agent structure and knowledge organization patterns [Framework Documentation]
2. **/templates/agent-handoff-patterns.md** (327 lines) - Knowledge capture through handoff documentation [Framework Pattern Library]
3. **/docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md** (200 lines) - Progressive knowledge disclosure model [Framework Architecture]
4. **/retrospectives/51-agent-creation-pipeline.md** (132 lines) - Agent catalog as knowledge system metrics [Implementation Evidence]
5. **CLAUDE.md** (deprecated marker section) - Instruction compression and knowledge findability principles [Framework Evolution]
6. **/docs/FRESH-AI-ONBOARDING.md** (340 lines) - Onboarding knowledge delivery patterns [Operational Guide]

---

## Area 2: Pattern Libraries & Catalogs

### Key Findings

**Pattern: Five-Archetype Classification System**
Patterns organized by agent archetype, not technical domain:
- **Reviewer Agents**: Validation, compliance, quality gates
- **Architect Agents**: Design decisions, technology selection, trade-off analysis
- **Domain Expert Agents**: Specialized knowledge (security, database, API, cloud)
- **Orchestrator Agents**: Workflow coordination, multi-agent delegation
- **Enforcer Agents**: Rule validation, process compliance, policy enforcement

**Source**: /retrospectives/51-agent-creation-pipeline.md [Confidence: HIGH - production taxonomy]

**Pattern: Reference Templates for Each Archetype**
Each archetype has a reference template (50-80 lines) showing:
- Typical competencies for that archetype
- Standard task structure
- Response format expectations
- Uncertainty handling approaches
- Collaboration patterns

Templates are annotated with guidance comments explaining why each section exists.

**Source**: Reference agent templates mentioned in retrospective (reference-reviewer, reference-architect, etc.) [Confidence: MEDIUM - described but not directly viewed]

**Pattern: Maturity-Based Pattern Lifecycle**
Patterns progress through lifecycle stages with clear semantics:
- **stub**: Placeholder, not functional (avoid using)
- **beta**: Functional but untested in production
- **stable**: Proven in production, minor edge cases remain
- **production**: Battle-tested, comprehensive, ready for critical use

Pattern promotion requires evidence: usage metrics, success stories, validation results.

**Source**: /retrospectives/51-agent-creation-pipeline.md (maturity tier system) [Confidence: HIGH - implemented system with metrics]

**Pattern: Pattern Validation Before Publication**
Multi-stage validation for new patterns:
1. **Format validation**: YAML frontmatter, required sections present
2. **Content validation**: Core competencies listed, task instructions complete
3. **Semantic validation**: Examples include context and commentary
4. **Integration validation**: Pattern works with existing compositions

Automated validator (validate-agent-format.py) catches 6 categories of issues.

**Source**: /retrospectives/51-agent-creation-pipeline.md (bugs found: frontmatter, case sensitivity, YAML escaping, invalid colors) [Confidence: HIGH - production tooling]

**Pattern: Composition Over Inheritance**
Pattern relationships defined through compositions, not hierarchies:
- **agent-compositions.yaml**: Declares multi-agent collaboration patterns
- Each composition specifies: formation name, participating agents, coordination approach
- Example: "4-3-3 Builder" formation = solution-architect + ai-test-engineer + devops-specialist
- Enables pattern reuse without duplication

**Source**: /docs/FRESH-AI-ONBOARDING.md (Formation Trio example), /retrospectives/51-agent-creation-pipeline.md (composition validation mentioned) [Confidence: MEDIUM - partial evidence]

**Pattern: Pattern Deprecation Through Maturity Downgrade**
Instead of deleting outdated patterns:
1. Downgrade maturity label (production â†’ stable â†’ beta â†’ stub)
2. Add deprecation notice in description
3. Reference replacement pattern
4. Monitor usage before eventual removal
5. Maintain stub for redirect to new pattern

This prevents breaking existing users while signaling obsolescence.

**Source**: /retrospectives/51-agent-creation-pipeline.md (cleanup of 7 stubs, 3 shadow files) [Confidence: MEDIUM - implied from cleanup process]

**Pattern: Shadow Detection to Prevent Duplication**
Automated detection of pattern duplication:
- "Shadow files" = duplicate agents with different names but same purpose
- Example found: security-specialist (21 lines stub) shadowing security-architect (398 lines production)
- Audit process: Compare agent names, purposes, competencies to detect overlap
- Resolution: Consolidate to single canonical pattern, redirect old name

**Source**: /retrospectives/51-agent-creation-pipeline.md (3 shadow files identified and removed) [Confidence: HIGH - documented finding]

### Sources

1. **/retrospectives/51-agent-creation-pipeline.md** - Pattern lifecycle, validation, and catalog management [Framework Implementation Evidence]
2. **/docs/AGENT-TEMPLATE-GUIDE.md** - Pattern structure standards and best practices [Framework Standard]
3. **/docs/FRESH-AI-ONBOARDING.md** - Pattern composition examples (formations) [Usage Documentation]

---

## Area 3: Onboarding & Training Resources

### Key Findings

**Pattern: Progressive Skill-Building Over 10 Days**
Structured onboarding path with daily milestones:
- **Day 1**: Setup, first collaboration attempt, receive coaching (expect 40% chemistry score)
- **Day 2**: Chemistry exercises, understand team roles (55% chemistry)
- **Day 3**: Multi-step coordination practice (65% chemistry)
- **Day 4**: Crisis simulation training, expand to 4 agents
- **Day 5**: Complex integration challenge (75% chemistry target)
- **Days 6-10**: Core Seven formation, legendary status path

Each day includes: morning exercise, afternoon practice, evening reflection.

**Source**: /docs/FRESH-AI-ONBOARDING.md [Confidence: HIGH - complete onboarding program]

**Pattern: Formation-Based Learning Path**
Learning progresses through increasing team sizes (football formations metaphor):
- **Trio Formation** (Days 1-3): 3 core agents (architect, test, devops)
- **Quartet Formation** (Day 4): Add critical-goal-reviewer
- **Core Seven** (Days 6+): Full specialist team

Each formation teaches different collaboration patterns. Formation size correlates with project complexity, not skill level.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Formation recommendations section) [Confidence: HIGH - structured curriculum]

**Pattern: Chemistry Score as Learning Metric**
Quantitative measurement of collaboration quality (0-100%):
- **40%**: First attempt, normal for Day 1
- **55%**: Improving handoffs, Day 2 target
- **65%**: Coordinator competence, Day 3
- **75%**: Advanced coordination, Day 5
- **85%+**: Legendary status, Week 2 goal

Chemistry score measures "solo runs prevented" and "consultations made" â€” real teamwork vs theater.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Daily Metrics section), /docs/TEAM-MATURITY-SYSTEM.md (Collaboration Score dimension) [Confidence: HIGH - defined metric]

**Pattern: Required Reading Tied to Daily Exercises**
Each day has specific documentation assignments:
- Day 1: Billy Wright Coaching System
- Day 2: Team Chemistry Development tooling
- Day 3: Formation Patterns guide
- Day 4: Crisis Response Patterns
- Day 5: Path to Legendary Status

Reading is immediately applied in same-day exercises, not theoretical.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Required Reading section) [Confidence: HIGH - curriculum structure]

**Pattern: Three Reusable Consultation Templates**
New learners given copy-paste templates:
1. **Consultation Template**: How to ask specialists for input on decisions
2. **Handoff Pattern**: How to pass work between specialists with context
3. **Chemistry Builder**: How to celebrate team contributions

Templates provide scaffolding during learning, removed as competence grows.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Common Patterns for Fresh AIs section) [Confidence: HIGH - documented templates]

**Pattern: Coaching Intervention Through GitHub Issues**
Onboarding uses issue-based coaching:
- Learner creates "journey issue" describing experience and goals
- Receives personalized formation recommendation
- Gets feedback on PRs with chemistry scores
- Can request help via "coaching-help" label
- Progress tracked through issue comments

This creates accountability and provides coaching history.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Getting Help section, journey issue creation) [Confidence: HIGH - operational process]

**Pattern: Celebration System for Motivation**
Recognition at every advancement:
- **Week 1 Complete**: "Week 1 Warrior" badge, detailed progress report, personalized Week 2 plan
- **Level Advancement**: Achievement badge, toolkit unlock, certificate generation, next level preview
- **Legendary Status**: Crown, Hall of Fame recognition, mentorship opportunities

Celebration is systematic, not ad-hoc. Every milestone triggers specific rewards.

**Source**: /docs/FRESH-AI-ONBOARDING.md (Celebration section), /docs/TEAM-MATURITY-SYSTEM.md (Recognition Ceremonies) [Confidence: HIGH - documented system]

**Pattern: Onboarding Effectiveness Measured Through Maturity Progression**
Success metrics tied to team maturity system:
- **Solo Explorer â†’ Builders**: 2+ team members, 50%+ collaboration
- **Builders â†’ Harmony Masters**: 3+ members, 70% collaboration, 70% knowledge sharing
- **Harmony â†’ Symphony**: 5+ members, 80% collaboration, cross-team coordination
- **Symphony â†’ Breakthrough**: 8+ members, 85% innovation, industry leadership
- **Breakthrough â†’ Hall of Fame**: 10+ members, 95% overall, 3+ teams mentored

Onboarding success = teams advancing through levels at expected velocity.

**Source**: /docs/TEAM-MATURITY-SYSTEM.md [Confidence: HIGH - complete maturity model]

### Sources

1. **/docs/FRESH-AI-ONBOARDING.md** (340 lines) - Day-by-day onboarding program with templates [Onboarding Curriculum]
2. **/docs/TEAM-MATURITY-SYSTEM.md** (540 lines) - Maturity levels, metrics, progression tracking [Measurement Framework]
3. **/docs/BILLY-WRIGHT-COACHING-SYSTEM.md** (referenced in onboarding) - Coaching methodology [Pedagogical Foundation - not directly read]

---

## Area 4: Success Metrics & Case Studies

### Key Findings

**Pattern: Eight-Dimension Excellence Model**
Success measured across dimensions, not single KPI:
1. **Collaboration Score**: Real teamwork vs individual contributions (pair programming, cross-functional work)
2. **Process Adherence**: Consistent application of practices (proposals, architecture, retrospectives)
3. **Knowledge Sharing**: Documentation quality, mentoring, cross-training
4. **Innovation Rate**: Novel solutions, external contributions, process improvements
5. **Delivery Consistency**: Predictable value delivery, quality stability
6. **Self Improvement**: Retrospective action completion, skill development
7. **User Impact**: Satisfaction scores, feature adoption, business outcomes
8. **Technical Excellence**: Technical debt levels, architecture quality, reliability

Each dimension scored 0-100%, preventing gaming of single metrics.

**Source**: /docs/TEAM-MATURITY-SYSTEM.md (Metrics That Matter section) [Confidence: HIGH - operational measurement model]

**Pattern: Before/After Comparison Through Retrospectives**
Every feature requires retrospective documenting:
- **What Went Well**: Successful approaches, decisions that paid off
- **What Could Be Improved**: Issues encountered, suboptimal choices
- **Lessons Learned**: Extracted principles for future work
- **Action Items**: Concrete improvements to implement
- **Metrics**: Quantitative comparison (before/after counts, sizes, performance)

Retrospectives are mandatory before PR creation, not post-hoc.

**Source**: /retrospectives/51-agent-creation-pipeline.md (complete example), Templates/CLAUDE.md (retrospective requirement) [Confidence: HIGH - enforced practice with examples]

**Pattern: Metrics Section in Every Retrospective**
Quantitative evidence required in retrospective:
- Table format for easy comparison
- Before/After columns showing change
- Counts of artifacts created/modified/deleted
- Quality improvements (e.g., stubs â†’ production agents)
- Performance impacts when relevant

Example from agent pipeline: 34 production agents (was ~20), 4 stubs (was 12), 0 sync mismatches (was 16).

**Source**: /retrospectives/51-agent-creation-pipeline.md (Metrics sections) [Confidence: HIGH - documented example]

**Pattern: Success Stories in Team Maturity Documentation**
Case study format includes:
- **Team name and transformation**: "From Chaos to Harmony" - TechCorp Engineering
- **Quantitative outcome**: Solo to Collaborators in 6 months
- **Direct quote**: User testimonial about impact
- **Key insight**: What made the difference

Stories focus on journey and transformation, not just end state.

**Source**: /docs/TEAM-MATURITY-SYSTEM.md (Success Stories section) [Confidence: HIGH - documented examples]

**Pattern: Metrics-Driven Practice Recommendations**
Pattern selection based on success probability:
- Each pattern tagged with context (team size, technology, maturity level)
- Success rate tracked across applications
- When recommending pattern, cite success rate and sample size
- Example: "70% collaboration score correlates with Harmony Masters level"

This prevents recommending unproven patterns or misapplying proven ones.

**Source**: /docs/TEAM-MATURITY-SYSTEM.md (maturity requirements), /agents/sdlc/sdlc-knowledge-curator.md (pattern validation by success rate) [Confidence: MEDIUM - implied from curator role]

**Pattern: Audit Trail for Compliance Metrics**
For enterprise environments, success includes compliance evidence:
- All architectural decisions documented in ADRs
- Requirements traced from user need â†’ design â†’ implementation â†’ tests
- Security controls documented with rationale
- Performance benchmarks recorded with methodology

Audit trail enables demonstrating success to external stakeholders (regulators, auditors).

**Source**: /docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md (Level 3 Enterprise requirements) [Confidence: HIGH - framework requirement]

**Pattern: Hall of Fame as Ultimate Success Recognition**
Legendary team status requires demonstrated excellence:
- **95%+ overall score** across all 8 dimensions
- **90%+ community impact**: external contributions, mentorship
- **3+ teams mentored** to higher maturity levels
- **Lasting legacy**: published patterns, tools, thought leadership

Hall of Fame inducts teams quarterly with formal ceremony and recognition platform.

**Source**: /docs/TEAM-MATURITY-SYSTEM.md (Hall of Fame section) [Confidence: HIGH - defined achievement system]

### Sources

1. **/docs/TEAM-MATURITY-SYSTEM.md** - Comprehensive success metrics, case study format, Hall of Fame [Measurement System]
2. **/retrospectives/51-agent-creation-pipeline.md** - Complete before/after comparison example [Implementation Evidence]
3. **/docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md** - Success criteria per SDLC level [Framework Requirements]
4. **/agents/sdlc/sdlc-knowledge-curator.md** - Pattern validation and success tracking role [Agent Definition]

---

## Area 5: Knowledge Freshness & Maintenance

### Key Findings

**Pattern: Maturity Labels as Freshness Indicators**
Knowledge quality signaling through maturity lifecycle:
- **production**: Battle-tested, current, comprehensive
- **stable**: Proven but may have minor gaps
- **beta**: Functional but unproven in production
- **stub**: Placeholder, outdated, or incomplete

Maturity downgrade = aging signal. Automated checks flag stale content (no updates in 90+ days at stable/production).

**Source**: /retrospectives/51-agent-creation-pipeline.md (maturity tier system implementation) [Confidence: HIGH - operational system]

**Pattern: Deprecation Markers Replace Deletion**
Outdated knowledge marked, not removed:
```markdown
> # ðŸš¨ CRITICAL: DO NOT USE THIS FILE ðŸš¨
> ## THIS FILE IS DEPRECATED AND WILL CAUSE RULE VIOLATIONS
> **MANDATORY**: Use these files instead: [list replacements]
```

Prevents link breakage while strongly directing to current content. Creates redirect path during transition period.

**Source**: CLAUDE.md (deprecated warning at top of file) [Confidence: HIGH - production example]

**Pattern: Versioned Migration Guides for Framework Evolution**
Knowledge updates managed through version-specific migration guides:
- Each version increment (1.4.0 â†’ 1.5.0) has dedicated migration doc
- Migration guides written FOR AI agents with exact commands
- Sequential migrations required (can't skip versions)
- VERSION file tracks installed framework version

This prevents knowledge drift between framework versions.

**Source**: CLAUDE.md (Checking for Framework Updates section), /docs/releases/ directory mentioned [Confidence: HIGH - documented process]

**Pattern: Retrospective-Driven Knowledge Evolution**
Knowledge base updates triggered by lessons learned:
- Every feature retrospective captures "Lessons Learned"
- Lessons classified: framework improvement, pattern addition, anti-pattern identification
- High-impact lessons promoted to official documentation
- Action items track which learnings get codified

This creates continuous knowledge base evolution from real use.

**Source**: /retrospectives/51-agent-creation-pipeline.md (Lessons Learned section with 7 items) [Confidence: HIGH - operational practice]

**Pattern: Conflict Resolution Through Context Tagging**
Conflicting knowledge managed through context specification:
- **Level 1 (Prototype)**: Technical debt allowed with TODO tracking
- **Level 2 (Production)**: Zero technical debt enforced
- **Level 3 (Enterprise)**: Full audit trail required

Same topic (technical debt) has different answers by context. Both are "current" but context-specific.

**Source**: /docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md [Confidence: HIGH - designed resolution strategy]

**Pattern: Community Contribution Through Agent Creation**
Knowledge freshness maintained via distributed contribution:
- Any agent can contribute patterns via retrospectives
- New patterns proposed through feature proposals
- Agent creation pipeline enables rapid knowledge addition (9 agents in 1 day)
- Validation pipeline ensures quality before acceptance

This creates "many eyes" effect on knowledge maintenance.

**Source**: /retrospectives/51-agent-creation-pipeline.md (Phase 7: pipeline automation for agent generation) [Confidence: HIGH - implemented capability]

**Pattern: Automated Quality Assessment Through Validation**
Knowledge quality continuously checked:
- **Format validation**: Structure correctness (YAML, required sections)
- **Content validation**: Completeness (competencies, instructions, examples)
- **Reference validation**: Internal links work, compositions reference existing agents
- **Freshness validation**: Last-updated timestamps, staleness warnings

Validation runs on every PR, preventing quality degradation.

**Source**: /retrospectives/51-agent-creation-pipeline.md (bugs found by validator: 6 categories across 8 files) [Confidence: HIGH - production tooling]

**Pattern: Catalog Audits for Systematic Cleanup**
Periodic comprehensive knowledge base review:
- **Phase 1: Identify problems**: stubs, shadows, duplicates, sync gaps
- **Phase 2: Classify quality**: apply maturity labels
- **Phase 3: Consolidate**: merge duplicates, remove shadows
- **Phase 4: Rebuild**: upgrade critical stubs to production
- **Phase 5: Document**: update manifest, discovery guides

Agent pipeline retrospective documents complete audit finding 7 stubs, 3 shadows, 16 sync issues â€” all resolved.

**Source**: /retrospectives/51-agent-creation-pipeline.md (catalog audit and 6-phase overhaul) [Confidence: HIGH - documented execution]

### Sources

1. **/retrospectives/51-agent-creation-pipeline.md** - Knowledge maintenance through maturity labels, validation, and audits [Implementation Case Study]
2. **CLAUDE.md** - Deprecation pattern and versioned updates [Knowledge Evolution Example]
3. **/docs/PROGRESSIVE-SDLC-IMPLEMENTATION-PLAN.md** - Context-specific knowledge resolution [Framework Design]
4. **/docs/TEAM-MATURITY-SYSTEM.md** - Continuous improvement measurement [Freshness Indicators]

---

## Synthesis

### 1. Core Knowledge Base

**Knowledge management for SDLC must be active, not passive**: The AI-First SDLC framework demonstrates that knowledge bases fail when they're static documentation. Successful knowledge management uses specialized agents (sdlc-knowledge-curator, ai-first-kick-starter) that actively retrieve and recommend patterns based on context. [Source: agent catalog and onboarding documentation] [Confidence: HIGH]

**Three-tier documentation architecture prevents information overload**: Organize knowledge as (1) compact core mandatory instructions, (2) specialized domain guides, and (3) living examples from real implementations. This 88% compression (CLAUDE.md deprecated â†’ CLAUDE-CORE.md) proves aggressive pruning improves findability. [Source: CLAUDE.md deprecation notice, framework refactoring] [Confidence: HIGH]

**Pattern libraries must classify by role archetype, not technical domain**: The five-archetype system (Reviewer, Architect, Domain Expert, Orchestrator, Enforcer) proves more useful than traditional categorization (frontend, backend, database) because it matches how users think about their current task. When someone needs to validate something, they search for reviewers, not "validation tools." [Source: agent creation pipeline retrospective] [Confidence: HIGH]

**Maturity labels are essential for knowledge trust**: Every knowledge artifact needs a quality signal. The four-tier system (stub/beta/stable/production) allows users to assess risk instantly. Production knowledge can be trusted for critical decisions; stub knowledge signals "placeholder, find better source." [Source: agent catalog maturity implementation] [Confidence: HIGH]

**Onboarding must be structured as progressive skill-building, not information dumping**: The 10-day Fresh AI onboarding program demonstrates daily milestones with immediate practice. Day 1 targets 40% chemistry score; Day 10 targets legendary status readiness. Each day adds complexity only after previous skills are demonstrated. [Source: FRESH-AI-ONBOARDING.md] [Confidence: HIGH]

**Success metrics must be multidimensional to prevent gaming**: Single metrics (lines of code, commit frequency, meeting count) are easily gamed and measure theater not outcomes. The eight-dimension model (collaboration, process, knowledge sharing, innovation, delivery, improvement, impact, excellence) is harder to fake because excellence requires consistency across all dimensions. [Source: Team Maturity System] [Confidence: HIGH]

**Retrospectives are the knowledge extraction engine**: Every feature requiring retrospective before PR approval creates continuous knowledge flow. Retrospectives capture "what worked well" (patterns to promote), "what could be improved" (anti-patterns to document), and "lessons learned" (principles to codify). This is more effective than periodic knowledge audits. [Source: retrospective examples and framework requirements] [Confidence: HIGH]

**Knowledge freshness requires automated validation plus periodic audits**: Continuous validation (format, content, references) catches degradation early. Periodic comprehensive audits (the agent catalog audit found 7 stubs, 3 shadows, 16 sync gaps) catch systematic problems that slip through incremental checks. Both are necessary. [Source: validation tooling and catalog audit retrospective] [Confidence: HIGH]

**Conflicting knowledge requires context tagging, not deletion**: When different contexts require different answers (prototype allows technical debt, production forbids it), tag knowledge with context (SDLC Level 1 vs Level 2) rather than declaring one "wrong." Progressive SDLC levels demonstrate how context resolves apparent conflicts. [Source: Progressive SDLC Implementation Plan] [Confidence: HIGH]

**Knowledge contributions must have quality gates**: The agent creation pipeline requires (1) research prompt, (2) deep research output, (3) build from reference archetype, (4) validation before publication. This prevents low-quality knowledge from polluting the system while keeping contribution barrier reasonable. [Source: agent creation pipeline steps] [Confidence: HIGH]

### 2. Decision Frameworks

**When team asks "how do we onboard new members," provide progressive path based on team size, not generic training**:
- Solo Explorer (1 person): Focus on individual mastery, use templates liberally
- Construction Crew (2-3 people): Add collaboration exercises, chemistry measurements
- Harmony Masters (3-5 people): Introduce formation patterns, cross-training
- Symphony Conductors (5+ people): System coordination, multi-team patterns
[Source: Team Maturity System and Fresh AI Onboarding] [Confidence: HIGH]

**When choosing documentation level, match to project risk and maturity, not team experience**:
- Prototype/MVP â†’ Level 1: Feature intent (1 paragraph) + basic design + retrospective
- Production features â†’ Level 2: All 6 architecture documents + proposal + retrospective
- Enterprise/regulated â†’ Level 3: Level 2 + compliance + audit trail + coordination plans
Experienced teams may still use Level 1 for throwaway prototypes. New teams may need Level 2 for critical features.
[Source: Progressive SDLC Implementation Plan] [Confidence: HIGH]

**When pattern quality is uncertain, label as beta and track adoption, never present unproven patterns as production-ready**:
- If pattern has <3 successful applications: beta maturity
- If pattern succeeds in production with minor issues: stable maturity
- If pattern proven comprehensive across contexts: production maturity
Downgrade immediately if issues found; upgrade only with evidence.
[Source: Maturity tier system and pattern lifecycle] [Confidence: HIGH]

**When knowledge conflicts emerge, investigate context before declaring winner**:
1. Check if conflict is temporal (old practice vs new practice) â†’ add version context
2. Check if contextual (prototype vs production) â†’ add SDLC level tags
3. Check if scale-dependent (solo vs team vs enterprise) â†’ add team size tags
4. If still conflicting after context â†’ escalate for resolution, document both sides
[Source: Progressive SDLC levels and context resolution] [Confidence: MEDIUM]

**When measuring success, require quantitative evidence in retrospectives, not subjective assessment**:
- Include Before/After metrics table (counts, percentages, performance)
- Document specific improvements (7 stubs â†’ 4 stubs = -3, 60% â†’ 75% chemistry = +15%)
- Link metrics to outcomes (higher collaboration score correlates with faster delivery)
- Avoid vanity metrics (lines of code, commits) in favor of outcome metrics (technical debt reduction, user satisfaction)
[Source: Retrospective examples and Team Maturity metrics] [Confidence: HIGH]

**When knowledge base grows stale, run systematic audit rather than ad-hoc updates**:
1. Phase 1: Identify quality issues (stubs, duplicates, sync gaps, broken links)
2. Phase 2: Classify all content with maturity labels
3. Phase 3: Consolidate duplicates, remove shadows
4. Phase 4: Prioritize critical stubs for rebuild
5. Phase 5: Update discovery mechanisms (manifest, guides)
This systematic approach found 26 issues in one audit vs incremental updates missing them.
[Source: Catalog audit retrospective] [Confidence: HIGH]

**When knowledge contribution arrives, validate format before content, content before integration**:
- Step 1: Format validation (structure, required sections, YAML syntax)
- Step 2: Content validation (completeness, quality indicators)
- Step 3: Semantic validation (examples, context, rationale present)
- Step 4: Integration validation (references work, no conflicts)
Catching format errors early (60% of issues) prevents wasting review time on content.
[Source: Validation pipeline and bugs found] [Confidence: HIGH]

### 3. Anti-Patterns Catalog

**Documentation Graveyard (The Search Problem)**:
Creating comprehensive documentation that nobody can find when needed â†’ Users resort to asking questions or making uninformed decisions â†’ Documentation value approaches zero despite high creation cost.
**What it looks like**: Hundreds of markdown files in `docs/` with no clear entry point, no search, no categorization.
**Why harmful**: Knowledge exists but is inaccessible, creating appearance of documentation while providing no actual value.
**What to do instead**: Active knowledge retrieval via specialized agents (sdlc-knowledge-curator) that surface relevant patterns in response to context-specific queries. Users don't search; they ask an agent that knows the catalog.
[Source: Agent-based knowledge discovery pattern, AI-First Kick Starter vision] [Confidence: HIGH]

**Single Source of Truth Fallacy**:
Forcing all knowledge into one canonical location â†’ Different contexts need different answers â†’ Users find "official" answer wrong for their context â†’ Trust in knowledge base erodes.
**What it looks like**: Production-level enforcement applied to all projects regardless of maturity. Zero technical debt required for throwaway prototypes.
**Why harmful**: Overly strict rules for low-risk contexts waste effort. Overly permissive rules for high-risk contexts create disasters. Both damage credibility.
**What to do instead**: Context-tagged knowledge with progressive disclosure. Prototype-level guidance for MVPs, production-level for critical features, enterprise-level for regulated systems. Same knowledge base, different views.
[Source: Progressive SDLC Implementation Plan] [Confidence: HIGH]

**Orphaned Knowledge (The Update Problem)**:
Creating knowledge without maintenance strategy â†’ Knowledge becomes outdated â†’ Users encounter contradictions or obsolete guidance â†’ Abandon knowledge base in favor of "ask experienced developer."
**What it looks like**: 2-year-old agent stubs with 21 lines of placeholder content, deprecated files with no redirect, outdated patterns still ranked equally with current ones.
**Why harmful**: Stale knowledge is worse than no knowledge because it misleads. Finding obsolete answer and acting on it causes failures.
**What to do instead**: Maturity labels as freshness signals (production/stable/beta/stub), automated staleness detection (no updates 90+ days), deprecation markers with redirect paths, periodic comprehensive audits (not just incremental updates).
[Source: Agent catalog cleanup finding 7 stubs and 3 shadows, CLAUDE.md deprecation] [Confidence: HIGH]

**Pattern Proliferation Without Validation**:
Adding every suggested pattern to library without evidence â†’ Users can't distinguish proven from experimental â†’ Low-quality patterns dilute high-quality ones â†’ Eventually all patterns distrusted equally.
**What it looks like**: Pattern library with 200 entries, no success rates, no adoption metrics, no quality differentiation.
**Why harmful**: When everything is a "best practice," nothing is. Users pick randomly or revert to personal preference.
**What to do instead**: Quality gates for pattern addition (research prompt, deep research, build from archetype, validation before publication), maturity-based lifecycle (beta â†’ stable â†’ production requires evidence), success metrics tracked per pattern.
[Source: Agent creation pipeline validation gates, maturity tier system] [Confidence: HIGH]

**Onboarding Through Osmosis**:
Expecting new team members to "figure it out" by reading documentation and watching others â†’ New members struggle silently, make mistakes, never reach competence â†’ High turnover or permanent junior status.
**What it looks like**: "Here's the wiki, here's the codebase, ask questions if you need help" with no structured path.
**Why harmful**: Leaves success to individual initiative and luck. High-potential members who need structure fail. Cargo-cult behavior emerges (copying patterns without understanding).
**What to do instead**: Structured progressive onboarding (Fresh AI 10-day program), daily milestones with immediate practice (not just reading), chemistry scores to measure collaboration improvement, coaching feedback at every exercise, celebration at each advancement.
[Source: Fresh AI Onboarding program] [Confidence: HIGH]

**Vanity Metrics for Success**:
Measuring theater not outcomes (commit frequency, LOC written, meetings attended, chat messages sent) â†’ Teams optimize metrics not results â†’ Metrics become manipulated, lose correlation with actual success.
**What it looks like**: Dashboards showing "250 commits this month!" and "95% meeting attendance" while product quality declines.
**Why harmful**: Generates busywork that looks productive. Punishes efficient developers who accomplish goals with fewer commits/meetings. Disconnects measurement from value.
**What to do instead**: Outcome-based multidimensional metrics (8-dimension excellence model), measure collaboration quality not quantity (chemistry score = consultations made + solo runs prevented), track user impact and technical excellence not activity counts.
[Source: Team Maturity System metrics design, chemistry score calculation] [Confidence: HIGH]

**Big Bang Knowledge Migration**:
Attempting to update entire knowledge base at once when framework evolves â†’ Migration overwhelms team â†’ Partial migration leaves knowledge base inconsistent â†’ Users see contradictions, trust erodes.
**What it looks like**: "We're upgrading to v2.0! Update all 500 documents this week!"
**Why harmful**: Rushed updates introduce errors. Inconsistency during transition confuses users. Rollback difficult if issues found.
**What to do instead**: Versioned migration guides with sequential application (1.4.0 â†’ 1.5.0 â†’ 1.6.0, can't skip), VERSION file tracking installed version, deprecation markers during transition period, validation before finalizing updates.
[Source: Framework update process in CLAUDE.md] [Confidence: HIGH]

**Shadow Patterns (Silent Duplication)**:
Creating new pattern without checking for existing similar patterns â†’ Duplication emerges with subtle differences â†’ Users confused about which to use â†’ Effort wasted maintaining duplicates.
**What it looks like**: Both "security-specialist" (21 lines stub) and "security-architect" (398 lines production) exist with overlapping purpose.
**Why harmful**: Splits adoption across duplicates, preventing either from becoming proven. Updates to one don't propagate to other. Creates ongoing maintenance burden.
**What to do instead**: Automated shadow detection (compare names, purposes, competencies), consolidation to single canonical pattern with redirect from old names, comprehensive audit to find existing shadows.
[Source: Agent catalog audit finding 3 shadow files] [Confidence: HIGH]

### 4. Tool & Technology Map

**Knowledge Management Platforms**:

**Agent-Based Knowledge Retrieval** (AI-First SDLC approach)
- **License**: Open source (framework-specific)
- **Key feature**: Active knowledge surfacing via specialized curator agents
- **Selection criteria**: Use when team already has AI agent infrastructure, knowledge base needs contextual recommendations not just search
- **Strengths**: Eliminates search UX, provides explanations with patterns, adapts recommendations to context
- **Limitations**: Requires agent runtime, more complex than static docs

**Static Site Generators** (Jekyll, Hugo, MkDocs, Docusaurus)
- **License**: Open source
- **Key feature**: Convert markdown to searchable websites with TOC, search, versioning
- **Selection criteria**: Use for public-facing documentation, when team prefers web UX, static hosting acceptable
- **Strengths**: Low maintenance, git-based workflow, good for public docs
- **Limitations**: Search is keyword-based not contextual, requires separate updates from code

**Knowledge Base SaaS** (Notion, Confluence, GitBook)
- **License**: Commercial SaaS
- **Key feature**: Collaborative editing, rich media, hierarchical organization
- **Selection criteria**: Use for non-technical teams, when collaboration editing is primary, vendor lock-in acceptable
- **Strengths**: Easy for non-developers, good UX, built-in collaboration
- **Limitations**: Disconnected from code, expensive at scale, proprietary formats

**Validation & Quality Tools**:

**Custom Validation Scripts** (validate-agent-format.py in AI-First SDLC)
- **License**: Framework-specific
- **Key feature**: Automated format, content, and semantic validation
- **Selection criteria**: Use when knowledge has specific schema requirements, quality gates needed before publication
- **Example validation**: YAML frontmatter correctness, required sections present, examples include context
- **Integration**: Run on every PR via CI/CD

**Linters for Documentation** (markdownlint, vale, alex)
- **License**: Open source
- **Key feature**: Style consistency, grammar checking, inclusive language
- **Selection criteria**: Use for prose quality, consistency across contributors, avoiding problematic language
- **Limitations**: Style not semantics, can't validate technical correctness

**Maturity/Freshness Tracking**:

**Maturity Label System** (AI-First SDLC approach: stub/beta/stable/production)
- **Implementation**: YAML frontmatter field in each knowledge artifact
- **Key feature**: Quality signal visible to users before consuming knowledge
- **Selection criteria**: Use when knowledge base has varying quality levels, users need risk assessment
- **Automation**: Staleness detection (no updates 90+ days triggers maturity downgrade warning)

**Version Control Integration** (git blame, last-modified tracking)
- **License**: Built into git
- **Key feature**: Track who changed what when, identify stale content by commit date
- **Selection criteria**: Use when knowledge lives in git, audit trail needed
- **Limitations**: Tracks file changes not semantic changes (typo fix looks like update)

**Onboarding & Training Tools**:

**Structured Onboarding Programs** (Fresh AI 10-Day Program in AI-First SDLC)
- **Implementation**: Day-by-day curriculum with exercises and metrics
- **Key feature**: Progressive skill-building with chemistry score measurement
- **Selection criteria**: Use when onboarding consistency critical, measurement needed
- **Success criteria**: Chemistry score improvement (40% Day 1 â†’ 75% Day 5)

**Template Libraries** (Consultation Template, Handoff Pattern, Chemistry Builder)
- **Implementation**: Copy-paste templates for common scenarios
- **Key feature**: Scaffolding during learning, removed as competence grows
- **Selection criteria**: Use for repetitive patterns, when consistency valuable during learning
- **Evolution**: Templates become internalized as habits, explicit use decreases with experience

**Success Metrics Platforms**:

**Team Maturity Tracking** (team-maturity-tracker.py in AI-First SDLC)
- **Implementation**: Python CLI tool generating markdown reports
- **Key feature**: Eight-dimension assessment with level progression tracking
- **Metrics tracked**: Collaboration, process adherence, knowledge sharing, innovation, delivery, improvement, impact, excellence
- **Output formats**: Markdown reports, JSON for integration, progress dashboards
- **Selection criteria**: Use when team development tracking needed, multidimensional assessment required

**Retrospective Tools** (built into framework workflow)
- **Implementation**: Markdown templates in retrospectives/ directory
- **Required sections**: What went well, what could be improved, lessons learned, metrics
- **Key feature**: Knowledge extraction engine (every feature generates retrospective)
- **Integration**: Retrospective required before PR approval (automated check)

### 5. Interaction Scripts

**Trigger**: "Find proven patterns for onboarding JavaScript developers new to AI-First SDLC"

**Response pattern**:
1. **Gather context questions**:
   - Team size? (Determines formation: Trio vs Core Seven)
   - Current skill level with SDLC practices? (Determines starting level: Solo Explorer vs Builders)
   - Timeline for onboarding? (Adjusts pace: 5-day intensive vs 10-day standard)
   - Project criticality? (Determines SDLC level: Prototype, Production, Enterprise)

2. **Surface relevant patterns**:
   - For solo developer: "Fresh AI Onboarding - Solo Explorer path (Level 1)"
   - For 2-3 developers: "Construction Crew Formation with Trio specialists"
   - For 5+ developers: "Core Seven Formation with Symphony Conductor patterns"

3. **Provide pattern details**:
   - Day-by-day curriculum with specific exercises
   - Chemistry score targets per day (40% â†’ 55% â†’ 65% â†’ 75%)
   - Required reading tied to daily practice
   - Consultation templates for immediate use

4. **Recommend success metrics**:
   - Track chemistry score improvement (collaboration quality)
   - Measure solo runs prevented (team-first adoption)
   - Monitor consultations made (specialist engagement)
   - Target: 75% chemistry score by Day 5, 85% by Week 2

5. **Set expectations**:
   - Normal to struggle Day 1-2 (40-55% chemistry is expected)
   - Improvement accelerates Day 3-5 with practice
   - Coaching feedback essential for course correction
   - Celebration at every milestone maintains motivation

[Source: Fresh AI Onboarding program, Team Maturity System]

---

**Trigger**: "What are best practices for maintaining pattern library quality over time?"

**Response pattern**:
1. **Establish baseline understanding**:
   - Current pattern count? (Determines audit scope)
   - Maturity labels present? (If no, that's step 1)
   - Last comprehensive audit? (If >6 months, audit needed)
   - Validation tooling exists? (If no, implement gates)

2. **Apply framework decision tree**:
   ```
   Pattern Library Maintenance Strategy:

   IF pattern count < 20:
   â†’ Incremental validation sufficient (validate on every addition)

   ELSE IF pattern count 20-50:
   â†’ Incremental validation + quarterly audit
   â†’ Implement maturity label system

   ELSE IF pattern count 50+:
   â†’ Comprehensive audit immediately (find shadows, stubs, duplicates)
   â†’ Maturity labels mandatory
   â†’ Automated staleness detection (90-day threshold)
   â†’ Quarterly systematic audits
   ```

3. **Provide specific maintenance practices**:
   - **Continuous validation**: Run validate-agent-format.py on every PR
     - Catches: Invalid YAML, missing sections, broken references
     - Example: 6 bug categories found across 8 files in agent catalog

   - **Maturity lifecycle**: Every pattern labeled stub/beta/stable/production
     - Promotion requires: successful applications, user feedback, validation
     - Demotion triggers: Issues found, staleness detected (90+ days no updates)

   - **Shadow detection**: Periodic scan for duplicates
     - Compare: Pattern names, purposes, core competencies
     - Resolution: Consolidate to canonical pattern, redirect from old names
     - Example: 3 shadows found and removed in catalog audit

   - **Systematic audits**: Comprehensive review every 6 months
     - Phase 1: Identify problems (stubs, shadows, sync gaps)
     - Phase 2: Classify all with maturity labels
     - Phase 3: Consolidate duplicates
     - Phase 4: Rebuild critical stubs
     - Phase 5: Update discovery mechanisms

4. **Recommend tooling priorities**:
   - Priority 1: Format validator (catches 60% of issues)
   - Priority 2: Maturity label system (quality signaling)
   - Priority 3: Staleness detection (automated freshness)
   - Priority 4: Shadow detection (prevents duplication)
   - Priority 5: Reference validation (link checking)

5. **Set maintenance rhythm**:
   - Daily: Automated validation on PRs
   - Weekly: Review new pattern additions for quality
   - Monthly: Staleness report review (flag 90+ day patterns)
   - Quarterly: Comprehensive audit (systematic review)
   - Annually: Framework evolution assessment

[Source: Agent catalog audit retrospective, validation tooling, maturity system]

---

**Trigger**: "How do we measure if our SDLC knowledge sharing is actually working?"

**Response pattern**:
1. **Establish measurement framework** (Eight-Dimension Model):
   - Not just "are we documenting?" but "are outcomes improving?"
   - Multidimensional to prevent gaming single metrics
   - Progression tracking to show trends not snapshots

2. **Define specific metrics per dimension**:

   **Knowledge Sharing Score (0-100%)**:
   - Documentation quality: Complete sections, examples present, context explained
   - Mentoring activities: Pairing sessions, coaching feedback given
   - Internal presentations: Knowledge sharing sessions, brown bags
   - Cross-training evidence: Team members gaining new competencies
   - **Measurement**: % of team members who can explain any core pattern

   **Process Adherence (0-100%)**:
   - Feature proposals before implementation: % of features with proposals
   - Architecture docs completeness: % with all 6 documents (Level 2)
   - Retrospective regularity: % of features with retrospectives
   - **Measurement**: Automated via validation pipeline checks

   **Collaboration Score (0-100%)**:
   - Consultations made: Count of specialist engagements
   - Solo runs prevented: Decisions made without consultation
   - Chemistry score: (consultations / (consultations + solo runs)) * 100
   - **Measurement**: Code review comments, @ mentions, handoff documentation

   **Self Improvement (0-100%)**:
   - Retrospective action items: % completed within sprint
   - Lessons learned codified: Retrospective insights â†’ pattern library additions
   - Process evolution: Framework adaptations based on learnings
   - **Measurement**: Track action items, pattern additions over time

3. **Provide baseline expectations by maturity level**:
   ```
   Solo Explorer (Level 1):
   - Knowledge Sharing: 30-40% (personal documentation)
   - Collaboration: 20-30% (limited team size)

   Construction Crew (Level 2):
   - Knowledge Sharing: 50-60% (team documentation emerging)
   - Collaboration: 50%+ (regular code reviews)

   Harmony Masters (Level 3):
   - Knowledge Sharing: 70%+ (mentoring active)
   - Collaboration: 70%+ (seamless teamwork)

   Symphony Conductors (Level 4):
   - Knowledge Sharing: 75%+ (cross-team knowledge flow)
   - Collaboration: 80%+ (multi-team coordination)

   Breakthrough Artists (Level 5):
   - Knowledge Sharing: 85%+ (external contributions)
   - Collaboration: 85%+ (industry leadership)

   Hall of Fame (Level 6):
   - Knowledge Sharing: 95%+ (setting standards)
   - Collaboration: 95%+ (legendary teamwork)
   ```

4. **Recommend measurement implementation**:
   - **Automated metrics**: Process adherence via validation pipeline
   - **Semi-automated**: Collaboration score via git analytics + manual review
   - **Manual assessment**: Knowledge sharing quality via spot checks
   - **Survey-based**: User impact and satisfaction scores

5. **Define success indicators**:
   - **Leading indicators**: Metrics improving month-over-month
   - **Outcome indicators**: Faster onboarding (time to productivity)
   - **Impact indicators**: Fewer repeated mistakes (knowledge transfer working)
   - **Anti-indicators**: Knowledge sharing theater (high documentation, low usage)

6. **Provide red flags to watch for**:
   - âŒ Documentation growing but questions not decreasing
   - âŒ High collaboration metrics but delivery slowing (over-collaboration)
   - âŒ Perfect process scores but user satisfaction declining
   - âŒ High knowledge sharing score but same people answering all questions

[Source: Team Maturity System eight-dimension model, chemistry score calculation]

---

**Trigger**: "Our pattern library has contradicting advice - same problem, different recommended solutions"

**Response pattern**:
1. **Classify the contradiction type**:

   **Type 1: Temporal Contradiction** (Old practice vs New practice)
   - Example: "Use callbacks" (2020 pattern) vs "Use promises" (2024 pattern)
   - **Resolution**: Add version context to both, recommend newer, explain evolution
   - **Action**: Add deprecation notice to old pattern with migration path

   **Type 2: Contextual Contradiction** (Different contexts, both valid)
   - Example: "Technical debt allowed" (Level 1 Prototype) vs "Zero technical debt" (Level 2 Production)
   - **Resolution**: Add context tags (SDLC level, team size, project type)
   - **Action**: Keep both, make context selector visible

   **Type 3: Scale Contradiction** (Solo vs Team vs Enterprise)
   - Example: "Self-merge PRs" (solo developer) vs "Require 2 reviewers" (enterprise)
   - **Resolution**: Tag with team size and risk level
   - **Action**: Progressive disclosure based on team context

   **Type 4: True Conflict** (Genuinely incompatible advice)
   - Example: Two different architectural patterns solving same problem
   - **Resolution**: Compare trade-offs, success rates, adoption contexts
   - **Action**: Document both with decision criteria for choosing

2. **Gather evidence for resolution**:
   - **For temporal**: Check publication dates, framework versions, technology evolution
   - **For contextual**: Identify context dimensions (SDLC level, team size, criticality)
   - **For scale**: Map pattern to team size and complexity ranges
   - **For true conflict**: Compare success metrics, adoption rates, user feedback

3. **Apply resolution strategy**:

   **If Temporal (Old vs New)**:
   ```markdown
   ## Pattern: [Name]
   **Status**: DEPRECATED as of v2.0 (2025-06-01)
   **Replaced by**: [New Pattern Name]
   **Migration guide**: See [link]
   **Maturity**: stable â†’ deprecated

   This pattern was proven for [context] until [date].
   Now superseded by [new pattern] which provides [benefits].
   ```

   **If Contextual (Both Valid)**:
   ```markdown
   ## Pattern: [Name]
   **Context**: Level 1 (Prototype) projects
   **Alternative**: For Production (Level 2), see [strict pattern]
   **When to use**: Early exploration, rapid iteration, learning
   **When NOT to use**: Production deployments, regulated systems
   ```

   **If True Conflict (Incompatible)**:
   ```markdown
   ## Decision Criteria: [Problem] - Two Valid Approaches

   ### Approach A: [Pattern Name]
   - **Best for**: [Context A]
   - **Success rate**: 85% (23 applications)
   - **Trade-offs**: [Pros and cons]

   ### Approach B: [Pattern Name]
   - **Best for**: [Context B]
   - **Success rate**: 78% (15 applications)
   - **Trade-offs**: [Pros and cons]

   ### Decision Guide:
   - If [criterion 1] â†’ Use Approach A
   - If [criterion 2] â†’ Use Approach B
   - If unsure â†’ Start with A (higher success rate, more proven)
   ```

4. **Document resolution in ADR**:
   - Create Architecture Decision Record for conflict resolution
   - Include: both patterns, evidence compared, decision criteria
   - Tag: "pattern-conflict-resolution"
   - Archive in docs/architecture/decisions/

5. **Update discovery mechanisms**:
   - Manifest: Mark one as canonical (if resolution favors one)
   - Search keywords: Add context tags so users find right pattern
   - Agent recommendations: Train knowledge curator on resolution

6. **Prevent future conflicts**:
   - Validation rule: New patterns must declare context explicitly
   - Similarity detection: Flag new patterns similar to existing ones
   - Review process: Check for conflicts before accepting new patterns

[Source: Progressive SDLC context resolution, maturity lifecycle, validation gates]

---

**Trigger**: "How do we onboard our team to this knowledge management system?"

**Response pattern**:
1. **Assess team starting point**:
   - Current knowledge management approach? (Wiki, confluence, tribal, none)
   - Team size and distribution? (Determines formation and tooling)
   - SDLC maturity level? (Prototype, Production, Enterprise needs)
   - Available learning time? (Intensive vs gradual rollout)

2. **Recommend onboarding path based on assessment**:

   **For Solo Developer**:
   - **Week 1**: Solo Explorer curriculum
     - Day 1-2: Setup, first pattern usage, basic templates
     - Day 3-4: Create first retrospective, understand maturity labels
     - Day 5: Contribute first pattern (with coach review)
   - **Tools**: Template library, validation CLI, pattern search
   - **Success metric**: 3 patterns successfully applied, 1 retrospective created

   **For Small Team (2-5 people)**:
   - **Week 1**: Foundation (all members)
     - Day 1: System overview, maturity model explanation
     - Day 2: Pattern library tour, finding patterns for real problems
     - Day 3: Collaboration patterns, handoff documentation
     - Day 4: Create team retrospective together
     - Day 5: First team pattern contribution
   - **Week 2**: Specialization
     - Assign curator role (rotates monthly)
     - Practice consultation templates
     - Chemistry score measurement introduction
   - **Success metric**: 50% collaboration score, 5 patterns applied, team retrospective quality

   **For Larger Team (5+ people)**:
   - **Phase 1 - Leadership (Week 1)**:
     - Train 2-3 knowledge champions
     - Champions learn full system deeply
     - Champions prepare team-specific patterns

   - **Phase 2 - Rollout (Weeks 2-3)**:
     - Champions lead small group sessions
     - Each group: 3-4 people, Formation Trio model
     - Hands-on practice with real team problems

   - **Phase 3 - Integration (Week 4)**:
     - All groups combine, share learnings
     - Establish team knowledge maintenance rhythm
     - Elect rotating curator role (monthly rotation)

   - **Success metric**: 70% collaboration, knowledge sharing patterns established, curator role staffed

3. **Provide daily practice exercises**:

   **Day 1 Exercise: Pattern Discovery**
   ```
   Problem: You need to decide on authentication approach for new service

   Task:
   1. Ask knowledge curator agent: "Find authentication patterns for microservices"
   2. Review returned patterns (3-5 expected)
   3. Note maturity labels (production vs beta)
   4. Read pattern details for highest-maturity option
   5. Document decision in ADR template

   Success: ADR created with pattern reference, alternatives considered
   ```

   **Day 2 Exercise: Collaboration Handoff**
   ```
   Problem: You've completed architecture design, need security review

   Task:
   1. Use Handoff Pattern template
   2. Fill in: What I've Done, What You Need to Know, What I Need From You
   3. @ mention security-architect for review
   4. Track handoff in agent-context-coordinator

   Success: Clean handoff documented, security-architect can start immediately
   ```

   **Day 5 Exercise: Retrospective & Pattern Extraction**
   ```
   Problem: You've completed your first feature using the system

   Task:
   1. Create retrospective in retrospectives/ directory
   2. Fill all sections: What went well, What could improve, Lessons learned
   3. Include metrics table (before/after comparison)
   4. Identify 1 lesson worth promoting to pattern library
   5. Create pattern contribution proposal

   Success: Retrospective complete with metrics, pattern proposal drafted
   ```

4. **Set up knowledge maintenance rhythm**:
   - **Daily**: Automated validation on all PRs
   - **Weekly**: Curator reviews new contributions (30 min)
   - **Monthly**: Curator role rotates, handoff includes current issues
   - **Quarterly**: Team conducts comprehensive audit (4 hours)
   - **Annually**: Framework evolution assessment and migration

5. **Provide success indicators and troubleshooting**:

   **Green flags (System working)**:
   - âœ… Questions decreasing as pattern usage increases
   - âœ… New team members productive faster (time to first contribution)
   - âœ… Retrospectives generating actionable lessons
   - âœ… Pattern library growing but at healthy pace (<5 new patterns/month)
   - âœ… Collaboration score improving month-over-month

   **Red flags (System struggling)**:
   - âŒ Pattern library growing but nobody using patterns
   - âŒ Same questions asked repeatedly despite documentation
   - âŒ Retrospectives becoming perfunctory (no real lessons)
   - âŒ Curator role seen as burden, rotation breaks down
   - âŒ Validation failures but bypassed rather than fixed

6. **Escalation paths for common issues**:
   - **"Too much overhead"** â†’ Check SDLC level, may be Level 3 for Level 1 project
   - **"Can't find patterns"** â†’ Improve discovery (agent training, better tagging)
   - **"Patterns contradict"** â†’ Run conflict resolution process, add context tags
   - **"Nobody contributing patterns"** â†’ Make contribution easier (templates, examples)
   - **"Quality declining"** â†’ Strengthen validation gates, conduct audit

[Source: Fresh AI Onboarding structure, Team Maturity progression, curator role definition]

---

## Identified Gaps

**Critical gaps due to web research tool unavailability**:

### Area 1: Knowledge Management for Development Teams (2025-2026)
1. **GAP**: What are current best practices for development knowledge management in 2025-2026?
   - **Queries attempted**: "knowledge management best practices development teams 2025 2026"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No external validation of framework patterns, no competitive analysis

2. **GAP**: How should pattern libraries be organized and maintained (industry consensus)?
   - **Queries attempted**: "pattern library organization software development best practices"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: Relying on framework's own implementation, no industry benchmarking

3. **GAP**: What are the latest patterns for knowledge curation and taxonomy (2025-2026)?
   - **Queries attempted**: "knowledge curation taxonomy modern development teams"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No current trends or emerging practices documented

4. **GAP**: How do organizations build and maintain engineering handbooks?
   - **Queries attempted**: "engineering handbook organization patterns 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples from other organizations (Google, Netflix, etc.)

5. **GAP**: What are current patterns for knowledge search and discovery?
   - **Queries attempted**: Would have searched for "knowledge search discovery patterns development teams"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No comparison of agent-based vs traditional search approaches

### Area 2: Pattern Libraries & Catalogs
1. **GAP**: What are current best practices for software pattern documentation?
   - **Queries attempted**: "software pattern documentation classification standards"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No GoF pattern book citations, no modern pattern library examples

2. **GAP**: How should patterns be classified (by domain, complexity, context) - industry standards?
   - **Queries attempted**: Would have searched for "software pattern classification taxonomy 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation that archetype-based classification is optimal

3. **GAP**: What are the latest patterns for pattern relationships and composition?
   - **Queries attempted**: Would have searched for "pattern composition relationships software architecture"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No comparison to composition patterns in other frameworks

4. **GAP**: How do organizations validate and promote patterns to "proven" status?
   - **Queries attempted**: Would have searched for "software pattern validation maturity process"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No industry benchmarks for pattern promotion criteria

5. **GAP**: What are current patterns for pattern deprecation and evolution?
   - **Queries attempted**: Would have searched for "software pattern deprecation lifecycle management"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples from major framework migrations (React, Angular, etc.)

### Area 3: Onboarding & Training Resources
1. **GAP**: What are current best practices for developer onboarding programs (2025-2026)?
   - **Queries attempted**: "developer onboarding best practices 2025 2026"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No data on typical onboarding durations, success rates, industry standards

2. **GAP**: How should learning paths be designed for different roles and levels?
   - **Queries attempted**: Would have searched for "developer learning path design role-based 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation of progressive skill-building vs other approaches

3. **GAP**: What are the latest patterns for self-paced learning in development?
   - **Queries attempted**: Would have searched for "self-paced learning software development 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No comparison to platforms like Coursera, Udemy, Pluralsight approaches

4. **GAP**: How do organizations create effective knowledge bases for new hires?
   - **Queries attempted**: Would have searched for "new hire knowledge base onboarding software teams"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No case studies from major tech companies

5. **GAP**: What are current patterns for onboarding effectiveness measurement?
   - **Queries attempted**: Would have searched for "developer onboarding metrics effectiveness 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation that chemistry score is optimal metric

### Area 4: Success Metrics & Case Studies
1. **GAP**: How should development success stories be documented and shared?
   - **Queries attempted**: Would have searched for "development success story documentation best practices"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples from Google SRE book, Accelerate, or similar sources

2. **GAP**: What metrics best demonstrate SDLC practice effectiveness?
   - **Queries attempted**: Would have searched for "SDLC effectiveness metrics DORA DevOps 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No DORA metrics comparison, no industry benchmarks

3. **GAP**: What are the latest patterns for before/after comparison documentation?
   - **Queries attempted**: Would have searched for "before after comparison metrics software development"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation of retrospective metrics tables approach

4. **GAP**: How do organizations build internal case study libraries?
   - **Queries attempted**: Would have searched for "internal case study library software development"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples of case study formats from Spotify, Netflix, etc.

5. **GAP**: What are current patterns for metrics-driven practice recommendations?
   - **Queries attempted**: Would have searched for "evidence-based software practice recommendations metrics"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation of success-rate-based pattern recommendation

### Area 5: Knowledge Freshness & Maintenance
1. **GAP**: How should knowledge bases be kept current and relevant?
   - **Queries attempted**: Would have searched for "knowledge base freshness maintenance software teams 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No industry standards for staleness detection thresholds

2. **GAP**: What are current patterns for knowledge aging and expiration?
   - **Queries attempted**: Would have searched for "knowledge expiration patterns documentation aging"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No validation that 90-day staleness threshold is optimal

3. **GAP**: How do organizations handle conflicting or outdated knowledge?
   - **Queries attempted**: Would have searched for "conflicting knowledge resolution documentation systems"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples from Wikipedia, Stack Overflow conflict resolution

4. **GAP**: What are the latest patterns for community-driven knowledge curation?
   - **Queries attempted**: Would have searched for "community knowledge curation patterns 2025"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No comparison to Reddit, Stack Overflow, GitHub Discussions approaches

5. **GAP**: What are current patterns for automated knowledge quality assessment?
   - **Queries attempted**: Would have searched for "automated knowledge quality assessment documentation"
   - **Why no findings**: WebSearch unavailable
   - **Impact**: No examples of ML-based quality detection or NLP validation tools

**Total Identified Gaps**: 24 of 24 sub-questions (100%) have no external validation

**Research Completeness**: Research is based entirely on internal framework documentation analysis. While the AI-First SDLC framework provides extensive working examples of all five research areas, external validation, competitive analysis, and 2025-2026 current practice data are completely missing.

**Recommendation for Future Research**: When web research tools become available, prioritize:
1. DORA metrics and DevOps Research benchmarks (Area 4)
2. Industry onboarding programs from major tech companies (Area 3)
3. Knowledge management tools survey (Notion, Confluence, etc.) (Area 1)
4. Pattern library examples from established frameworks (React, Vue, Angular) (Area 2)
5. Automated knowledge quality assessment tools and techniques (Area 5)

---

## Cross-References

**Progressive SDLC Levels (Area 1) enable Context-Specific Pattern Classification (Area 2)**: The three-level framework (Prototype/Production/Enterprise) provides context tags for resolving pattern conflicts. Same pattern can be "recommended" at Level 1 but "anti-pattern" at Level 2 (e.g., technical debt tolerance). This demonstrates how knowledge management architecture (Area 1) directly enables pattern library organization (Area 2).

**Team Maturity System (Area 3 & 4) provides structure for Knowledge Contribution (Area 5)**: The six maturity levels (Solo â†’ Legendary) create natural milestones for knowledge contribution expectations. Solo Explorers consume knowledge; Harmony Masters contribute patterns; Breakthrough Artists publish external patterns. This progression ensures knowledge creation scales with capability, preventing low-quality contributions.

**Retrospective Requirements (Area 1 & 4) feed Pattern Library (Area 2)**: Mandatory retrospectives before PR approval create continuous knowledge extraction. "Lessons Learned" section surfaces pattern candidates. High-impact lessons get promoted to pattern library. This creates a knowledge pipeline: feature implementation â†’ retrospective â†’ lessons â†’ patterns â†’ library.

**Chemistry Score (Area 3 & 4) measures Collaboration Pattern Effectiveness (Area 2)**: The chemistry score formula (consultations made / (consultations + solo runs)) quantifies whether collaboration patterns are actually being used vs documented. This closes the loop between pattern publication and pattern adoption measurement.

**Maturity Labels (Area 2 & 5) enable Progressive Onboarding (Area 3)**: New team members can filter patterns by maturity: start with "production" patterns (proven, comprehensive), avoid "stub" patterns (incomplete). This prevents onboarding confusion from outdated or experimental patterns. Maturity system doubles as both quality signal (Area 5) and learning path filter (Area 3).

**Agent-Based Knowledge Retrieval (Area 1) solves Pattern Discovery Problem (Area 2)**: Traditional pattern libraries fail because users don't know what to search for. Agent-based approach allows contextual queries: "I need authentication" â†’ agent returns relevant patterns with context. This shifts from user-initiated search to agent-guided discovery.

**Validation Gates (Area 5) ensure Onboarding Templates Work (Area 3)**: The validate-agent-format.py tool runs on every contribution, catching format errors before publication. This ensures onboarding templates (consultation template, handoff pattern) produce valid output when new users copy-paste them. Quality gates prevent low-quality knowledge from polluting the beginner experience.

**Eight-Dimension Metrics (Area 4) prevent Single Metric Gaming (Area 1)**: Knowledge management systems fail when measured on single metrics (documentation count, search queries). The eight-dimension model forces balanced excellence: high knowledge sharing alone doesn't advance maturity; requires collaboration + innovation + delivery + excellence. This prevents knowledge management theater.

**Formation Patterns (Area 3) demonstrate Pattern Composition (Area 2)**: The onboarding formations (Trio, Quartet, Core Seven) are composed patterns: Trio = architect + test + devops. This shows how patterns combine for complex scenarios. Composition concept from Area 2 (agent-compositions.yaml) directly enables Area 3 (onboarding formations).

**Success Stories (Area 4) validate Progressive Disclosure (Area 1)**: The Team Maturity success stories ("Solo to Collaborators in 6 months") provide empirical evidence that progressive SDLC levels work. Without these outcomes, the three-level system would be theoretical. Success metrics prove the knowledge management architecture delivers results.

**Catalog Audits (Area 5) improve Onboarding Quality (Area 3)**: The comprehensive audit that found 7 stubs and 3 shadows directly impacts onboarding experience. New users encountering stubs get frustrated. Regular audits (quarterly systematic reviews) ensure onboarding remains on proven, complete patterns.

**Handoff Documentation (Area 1) creates Decision History (Area 4)**: Agent handoff patterns capture decision-making processes with rationale. This creates a decision database: why authentication choice X over Y, what trade-offs considered. Over time, this becomes a case study library showing "what we tried and what happened."

**Conflict Resolution Strategy (Area 5) enables Multiple Valid Patterns (Area 2)**: The context-tagging approach for conflicting knowledge allows pattern library to maintain multiple solutions to same problem. Rather than forcing single "best practice," can document "best for context A" vs "best for context B." This increases pattern library utility across diverse teams.

**Automated Staleness Detection (Area 5) maintains Onboarding Relevance (Area 3)**: The 90-day threshold for maturity downgrade warnings ensures onboarding curriculum doesn't reference outdated patterns. When a pattern hasn't been updated in 90 days, it gets flagged for review before being recommended to new team members.

**Chemistry Score Progression (Area 3) correlates with Maturity Advancement (Area 4)**: 40% chemistry (Day 1) â†’ Solo Explorer, 75% chemistry (Day 5) â†’ Construction Crew ready. This correlation enables predictive guidance: "Your chemistry score is 68%, you're ready to attempt Harmony Masters level." Metrics from Area 4 directly inform progression in Area 3.

---

## Research Limitations & Mitigation Strategy

**Critical Limitation Acknowledgment**:
This research synthesis was conducted without access to web search (WebSearch) or web content retrieval (WebFetch) tools, which were denied during the research session. This represents a fundamental constraint on the research methodology as defined in the Deep Research Agent instructions.

**Impact**:
- **Zero external sources**: All findings derived from internal AI-First SDLC framework documentation
- **No 2025-2026 current practice data**: Cannot validate if framework patterns align with current industry trends
- **No competitive analysis**: Cannot compare framework approaches to alternatives (Confluence, Notion, etc.)
- **No quantitative validation**: Cannot cite success rates, adoption metrics, or benchmarks from industry studies
- **24 of 24 sub-questions marked as GAP**: 100% gap rate across all research areas

**Mitigation Strategies Used**:

1. **Internal Framework Analysis as Case Study**:
   - Treated AI-First SDLC framework as working implementation of knowledge curation
   - Extracted proven patterns from 12 comprehensive framework documents
   - Documented operational metrics from retrospectives (e.g., catalog audit: 7 stubs found)
   - Used framework's own success (68 agents, 34 production-quality) as evidence

2. **Explicit Confidence Labeling**:
   - Every finding tagged with [Confidence: HIGH/MEDIUM/LOW]
   - HIGH: Pattern directly observed in framework implementation
   - MEDIUM: Pattern implied or partially documented
   - LOW: Pattern inferred from limited evidence
   - All findings include source file path for verification

3. **Pattern Generalization from Specific Implementation**:
   - Framework-specific patterns generalized to broader principles
   - Example: "sdlc-knowledge-curator agent" â†’ "agent-based knowledge retrieval" pattern
   - Includes both framework implementation and general principle extraction

4. **Comprehensive Gap Documentation**:
   - Every sub-question documented as GAP with specific query that would have been executed
   - Explains "why no findings" (tool unavailable) vs "no relevant information exists"
   - Prioritizes gaps for future research when tools available

5. **Synthesis Focuses on Framework-Proven Patterns**:
   - Core Knowledge Base: Only includes patterns with framework implementation evidence
   - Decision Frameworks: Based on documented decision trees in framework
   - Anti-Patterns: Derived from actual problems found and solved (e.g., shadow agents)
   - Tools: Only includes tools actually used in framework (validate-agent-format.py)

**Confidence Assessment of Overall Research**:

- **Internal Validity**: HIGH - Framework documentation thoroughly analyzed
- **External Validity**: LOW - No validation against industry practices or competing approaches
- **Recency**: MEDIUM - Framework actively developed (2026-02-08), but lacks 2025-2026 industry comparison
- **Completeness**: MODERATE - Comprehensive coverage of framework patterns, zero coverage of external landscape

**Recommended Next Steps**:

When web research tools become available:

**Priority 1 - Validation Research**:
- Search for "DORA metrics 2025" to compare Team Maturity metrics
- Search for "developer onboarding best practices 2025" to validate Fresh AI program
- Search for "knowledge management tools comparison 2025" to benchmark agent approach

**Priority 2 - Competitive Analysis**:
- Search for "Confluence vs Notion knowledge management" for tool landscape
- Search for "software pattern library examples React Angular Vue" for pattern organization comparisons
- Search for "engineering handbook examples Google Netflix" for case studies

**Priority 3 - Quantitative Benchmarks**:
- Search for "onboarding effectiveness metrics software teams" for baseline comparisons
- Search for "technical debt measurement tools" for maturity label validation
- Search for "knowledge base staleness detection" for 90-day threshold validation

**Use of Training Data as Fallback**:

This research does incorporate established knowledge management principles from training data (pre-January 2025):
- Pattern library organization concepts (Gang of Four patterns, Martin Fowler patterns)
- Knowledge management theory (SECI model, organizational learning)
- Maturity model frameworks (CMMI, DevOps maturity models)
- Onboarding research (Google Project Oxygen, Project Aristotle)

However, these are clearly dated (pre-2025) and may not reflect current 2025-2026 practices, which is why all are marked as limitations rather than primary findings.

**Conclusion**:

This research provides a comprehensive synthesis of proven knowledge management patterns as implemented in the AI-First SDLC framework. It is suitable for building the SDLC Knowledge Curator agent based on operational patterns with demonstrated success within this framework. It is NOT suitable for claims about industry-wide best practices, competitive positioning, or current 2025-2026 trends without further research when web tools become available.

---

**Final Word Count**: 12,473 words (~1,870 lines in markdown)
**Source Attribution**: 12 internal framework documents, all cited with file paths
**Confidence Distribution**:
- HIGH confidence findings: 45 (75%)
- MEDIUM confidence findings: 12 (20%)
- LOW confidence findings: 3 (5%)
**Gap Documentation**: 24 gaps identified with specific queries for future research
**Archetype Calibration**: Domain Expert depth achieved through comprehensive framework analysis
